{"version":"1","records":[{"hierarchy":{"lvl1":"Coding & Writing Sample"},"type":"lvl1","url":"/gaussianprocesses","position":0},{"hierarchy":{"lvl1":"Coding & Writing Sample"},"content":"","type":"content","url":"/gaussianprocesses","position":1},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl2":"Context for this coding & writing sample"},"type":"lvl2","url":"/gaussianprocesses#context-for-this-coding-writing-sample","position":2},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl2":"Context for this coding & writing sample"},"content":"I presented a version of this notebook on Tuesday, April 29th to \n\nFernando Perez’s research group at UC-Berkely as part of an ongoing biweekly collaboration for cross disciplinary open science. I’ve modified it slightly to:\n\nAdd context that I had previously noted verbally\n\nReduce the computational complexity of the example so that it can run on a single \n\nbinder instance\n\nCreated a \n\nrepository so that software dependencies can be built automatically, and the data is present for the binder instance\n\nThis notebook can be run for free from any web browser be clicking the following binder link: \n\n\n\nGaussian Processes (GPs) are a machine learning technique that has wide application-- including \n\nBayesian estimation of parameter weights for neural networks. This coding sample provides an introduction to the background and basics around GPs, and unifies some\ndisparate terminology used by Data Scientists, Statisticians, and Earth Scientists.\n\n","type":"content","url":"/gaussianprocesses#context-for-this-coding-writing-sample","position":3},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl2":"Gaussian Processes Prediction"},"type":"lvl2","url":"/gaussianprocesses#gaussian-processes-prediction","position":4},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl2":"Gaussian Processes Prediction"},"content":"Terminology in science is often confusing. For instance, Principle Component Analysis (PCA) and Empirical Orthogonal functions (EOF) are functionally the same numerical method, with EOFs being the preferred term in Atmospheric communities and PCA being the more general term used in broader statistical discussions. Similarly, Gaussian Processes is the general term used in physics and statistical learning for what the geosciences has traditionally termed ‘kriging’, and which some branches of math refer to as Wiener–Kolmogorov prediction.\n\nGaussian Processes (GP) are perhaps best thought of as a collection of enhancements to the traditional kriging system. They are functionally equivalent to kriging (\n\nWilliams (1998)), and differ only in implantation details and slightly in philosophy. The GP philosophy treats interpolation as model selection from an infinite number of functions (\n\nRasmussen & Williams (2005)).\n\nThe model of covariance determines which kinds of functions are in the prior. Observations then are used to select functions from the prior which are compatible with those observations. The distribution of those selected functions provide uncertainty estimation\n\nAlthough stated somewhat differently, this Bayesian view fits with \n\nMatheron’s discussion of random fields, and is a close match to \n\nJournal’s description of projection into various solution spaces. These ‘prior’ functions are not required to be linear, as the figure below demonstrates:\n\nAs number of predictions ‘P’ grows to be large above, the mean of all ‘P’ approaches the best estimate, with the variance of predictions indicating a confidence bound (i.e., the std. dev. of predictions)\n\nThe primary object of interest in kriging is the variogram, which is the main method by which practitioners fit a covariance function. In contrast, the primary object of interest in GPs is the kernel, which is directly equivalent to covariances. The kernel formulation has significant computational savings, enabling solving of high dimensional kernel functions in low dimensional space via the kernel trick (i.e., computing only the inner products); the kernel formulation also admits easy translation to Fourier based methods when interpolating regular spaces (\n\nWilson & Adams (2013)). Kernel architecture is flexible, as valid kernels can be added or multiplied to form new synthesis kernels (\n\nCressie & Huang (1999)). For instance, non-stationary kernels can be combined with stationary kernels and white noise kernels, such that large spatial trends, local correlation and structure, and intrinsic random noise and error can be modeled and specified as discrete submodules and functions-- \n\nsee here for a practical example using Mauna Loa data in scikit-learn. Kernels can also be bounded for more efficient computation—for instance, spherical kernels give rise to sparse gram matrices that can be reordered and solved by efficient sparse solvers; the Matérn kernel (\n\nMatérn, 1960) can be modified to do the same. Separable non-stationary kernels can be written as a Kronecker tensor product (\n\nGenton (2002)), and solved in circulant matrixes by efficient Fourier methods (\n\nWilson & Nickisch (2015)).\n\nAlthough Gaussian Processes is the more general statistical term for covariance methods of prediction, the practice and term of kriging predates Gaussian Processes. Thus, to explain the history and theory of Gaussian Processes, we’ll start with kriging first.\n\n","type":"content","url":"/gaussianprocesses#gaussian-processes-prediction","position":5},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"What is kriging (informally)?","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses#what-is-kriging-informally","position":6},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"What is kriging (informally)?","lvl2":"Gaussian Processes Prediction"},"content":"Generally speaking, most people use kriging as an interpolator. Classic interpolators can be split into ‘estimators’ and ‘predictors’; this distinction (in line with \n\nCressie, 1990) mainly applies to highlight that ‘estimators’ provide an estimate of the surface, while ‘predictors’ assign a prediction that is accompanied by measures of confidence:\n\nEstimators including nearest neighbor, bilinear interpolation, and cubic spline (bottom), as compared with kriging predictions using three different kernels (top). The middle row shows the variance of the kriging prediction, which shows how confident we are of the prediction at that location\n\nTraditional ‘estimators’ are of the fairly boring type that most are fairly familiar with— canonical examples are bilinear interpolation, Inverse Distance Weighting (IDW), cubic convolution, etc. Other non-traditional examples of ‘estimators’ include things like nearest and natural neighbor interpolation, which are appropriate for specific domain cases when you want to preserve original data values. If your goal is just interpolation and you have sufficient data, using an estimator is perfectly fine-- and fast! However, if your data is sparse, kriging starts to become more appealing:\n\nThere is less data, which is more widely spaced, so the uncertainty shown in the middle row has increased\n\nThis sparse estimation capability is why kriging was originally developed. Kriging is named after Danie G. Krige, who’s 1951 master’s thesis developed and described what we call ‘Simple Kriging’. Danie Krige was a prospector looking for gold in South Africa, an application that had sparse input measurements, and expensive sampling. Krige’s method was coined ‘kriging’ by the early 1960’s by French mathematician Georges Matheron, who formalized and expanded the method.\n\nOf course, the other case where kriging is appealing is when you need to use the method as a predictor.\n\n","type":"content","url":"/gaussianprocesses#what-is-kriging-informally","position":7},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"Covariance, Gold, and the variogram model","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses#covariance-gold-and-the-variogram-model","position":8},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"Covariance, Gold, and the variogram model","lvl2":"Gaussian Processes Prediction"},"content":"\n\nTraditional kriging defines a model of covariance by fitting a function to the empirical ‘variogram’, which plots the variance between observations as a function of distance between those observations. The variogram ‘sill’ refers to variance between uncorrelated samples, and the ‘range’ is the lag-distance at which this paired observation decorrelation occurs. The ‘nugget’ refers to the intrinsic variance (i.e., the observational uncertainty) at distance zero-- that is, the variance of a single point observation with itself. The term ‘nugget’ is literally referring to ‘gold nugget’ in the context of prospecting; i.e., finding a ‘nugget’ of gold which has been displaced from the source gold deposit... just because a ‘nugget’ is present at a location, does not guarantee that you are coincident with the gold deposit that generated that observation; however, the expectation is that the nugget is spatially close to the ore that it came from!\n\nMathematically, specifying a ‘nugget’ value (or alpha in GP terminology) is effectively adding a constant to the diagonal of the covariance matrix such that each observation has some level of variance with itself. Doing this allows flexibility for the mean of the predictions to not intersect all of the observation points-- if covariance is set to unity along the diagonal, then any estimation will be a ‘rubber sheeting’ that ensures the output mean prediction passes through the original data points. Of course, we can also specify covariance per observation rather than as a constant along the diagonal, and selectively down weight low confidence observations.\n\n","type":"content","url":"/gaussianprocesses#covariance-gold-and-the-variogram-model","position":9},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"What I use Gaussian Processes for","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses#what-i-use-gaussian-processes-for","position":10},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"What I use Gaussian Processes for","lvl2":"Gaussian Processes Prediction"},"content":"My science background is Earth Science, specifically glaciology and hydrology around the polar ice systems. Much of what we know about glacial hydrology is from remote sensing observations, which we use machine learning to process.\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom osgeo import osr\nimport pyproj\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel, RBF\n\ndata = pd.HDFStore(\"./Z62SU06\" + \".h5\", 'r') # ICESat data\n\nThe filename Z62SU06 refers to Zwally drainage basin 6.2 from Summer of 2006, and the observations are from the first space based laser altimeter, the \n\nNASA ICESat mission. Here’s where basin 6.2 is in Greenland:\n\ncoordlist = data['coords'] # Needed for distance calculations / plotting\ndf_indices = data['indices'] # UUID to link to observations to slope retrievals\ndf_corr = data['corr'] # Filter and weight data\n\ngslope = np.load('./General_slopes.npy') # Sub ICESat footprint slope retrievals\nmean_slope = np.zeros(len(coordlist))\nstd_slope = np.zeros(len(coordlist))\n\n# Merge and aggregate slope\nfor i in tqdm(range(len(coordlist))):\n    sweights = np.array(df_corr.iloc[i].values)\n    weightidx = np.isfinite(sweights)\n    sweights = np.abs(sweights[weightidx])\n    sidx = df_indices.iloc[i].values[weightidx]\n    sidx = np.array(sidx, dtype=int)\n    mean_slope[i] = np.average(gslope[sidx], weights=sweights)\n    std_slope[i] = np.std(gslope[sidx])\n\nThis example is meant to be larger than a ‘toy’ dataset, but still small enough to interact with and demonstrate. Our input data consist of 23,359 observations which are within footprint slope retrievals from the first ICESat mission (which flew from 2003 to 2009, although laser power was strongest in the first three years).\n\nHere’s what the data looks like:\n\nfig, ax = plt.subplots()\nim = ax.scatter(coordlist.lon, coordlist.lat, c=mean_slope, s=std_slope**2)\nim.set_clim(min(mean_slope), max(mean_slope))\nplt.xlabel(\"Longitude\"), plt.ylabel(\"Latitude\")\nfig.colorbar(im, ax=ax, label=\"Degrees of slope \\n (points scaled by retrieval variance)\")\nplt.show()\n\nEach ‘line’ above is an orbital pass, also called a ‘track’. This is an interesting case of geostatistics because the sampling is quite odd-- the along track resolution is dense, but the across track resolution is sparse. Most of the code is actually setting up the source and target coordinate systems for the model. The input observational data is irregularly spaced and in lat/lon coordinates (one degree of Latitude is 110km), but we want our distance measurements to be in Euclidean space and output to a grid.\n\n# Projection object\nds = osr.SpatialReference()\nds.ImportFromEPSG(3411) # polar projection\nds.ExportToProj4()\np2 = pyproj.Proj(ds.ExportToProj4())\n\n# Observations to meters from lat / lon\nX, Y = p2(np.array(coordlist.lon),\n          np.array(coordlist.lat))\n# To vector form sklearn\ncoords_m = np.ones((len(coordlist),2))\ncoords_m[:,0] = X\ncoords_m[:,1] = Y\n\nOur input (source) data was already filtered to drainage basin 6.2, but we also want to ‘clip’ the (target) results to a drainage basin. This is for two reasons-- first, the drainage basin boundaries are at zero surface slope and provide a physically meaningful way to bound, divide, and tile the analysis. And second, covariance functions are expensive, so clipping and masking the edges of our area of interest helps things run faster... it also makes the output look good!\n\n# Drainage basin boundries\nzwally = pd.read_csv('./GrnDrainageSystems_Ekholm.txt',\n                     sep=r\"\\s+\", names=['basin', 'lat','long'])\nbasin = '6.2'\nLL = zip(zwally[zwally.basin == float(basin)].long,\n         zwally[zwally.basin == float(basin)].lat)\nLL = list(LL)\n# Convert basin boundries to meters from lat / lon\npX, pY = p2(np.array(LL)[:,0],\n            np.array(LL)[:,1])\n# Polygon object for masking\nZ = mpl.path.Path(list(zip(pX, pY)))\n\n# Setting up kriging grid\nx1, y1 = np.meshgrid(np.r_[round(min(pX), -2) - 2500:round(max(pX), -2) + 2500:5000],\n                     np.r_[round(min(pY), -2) - 2500:round(max(pY), -2) + 2500:5000])\nkcoords = np.vstack([x1.ravel(),y1.ravel()]).T\n\n# Masking kriging grid\nZidx = Z.contains_points(kcoords[:])\ntarget_c = kcoords[Zidx] # Target coordinates\nx1.shape, len(target_c)\n\nAt this point, we’ve setup a 5-km grid to predict on that’s 108 by 110 (a little over 500km per side), which after masking has 5,600 target coordinate locations to predict at.\n\nOur source observations are too large for a binder instance to use for the GP model, which is n-squared in memory complexity with number of observations. We subsample by randomly selecting 4,000 observations to fit our model to:\n\nrng = np.random.default_rng() # numpy sampling API\nnp.random.seed(12)\nind = np.array(range(0,len(coordlist)), dtype=int) # index values to map to observations\nidxs = np.random.choice(ind, size=4000) # randomly pick 4,000 indices\n\nThe actual Gaussian Process model specification is fairly short:\n\n%%time\nnoise = WhiteKernel(noise_level=.5)\nrbf = 3 * RBF(length_scale=[80000,80000])\nkern = 2.0 * Matern(length_scale=[80000,80000], length_scale_bounds=(1e3, 1e6),\n                        nu=0.5)\ngp1 = GaussianProcessRegressor(kernel=rbf+kern+noise, alpha=std_slope[idxs]**2, optimizer=None).fit(coords_m[idxs], mean_slope[idxs])\n\nWe are specifying a Matern kernel to calculate and estimate our local covariance structure, which is defined by the single parameter nu; this parameter controls how smooth the functions are within our prior distribution of possible surface functions. It is set at 0.5 (i.e., ‘1 / 2’, equivalent the absolute exponential kernel) to allow rough, non-differentiable candidate functions; a nu=1.5 (3 / 2) will select from functions that are once differentiable, nu=2.5 (5 / 2) will select from functions that are twice differentiable, and nu=inf will converge to the infinitely differentiable RBF kernel. In addition to the Matern kernel, I’ve also added an RBF kernel to model the large scale gradient of slope as it decreases moving inland, and a WhiteKernel to reflect that we expect some noise in the input data.\n\nThe length_scale parameter would typically be optimized during model fit based on the spatial structure of the data... however, because our input data is heavily unbalanced (i.e., dense in the along track direction, sparse in the across track), hyper-parameter optimization sets a length scale that is too short, so it is fixed at 80km, with similar settings applied for the length_scale_bounds.\n\nAfter model specification, at fit time we provide the coordinates, and slope values, along with an alpha that is set to variance of slope retrieval (i.e., estimates of variance per observation).\n\nWe can get the mean surface estimate and target-to-target covariances with the following:\n\n%%time\ny_mean, y_cov = gp1.predict(target_c, return_cov=True)\n\nWe’ll return to plot the results after discussing what is actually happening in the cell above.\n\n","type":"content","url":"/gaussianprocesses#what-i-use-gaussian-processes-for","position":11},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"What is kriging (formally)?","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses#what-is-kriging-formally","position":12},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"What is kriging (formally)?","lvl2":"Gaussian Processes Prediction"},"content":"Mathematically, kriging is the Best Unbiased Linear Predictor (\n\n(Journel (1977) ; \n\nCressie (1990) ; \n\nHandcock & Stein (1993)), and provides the Best Linear Unbiased Prediction, or BLUP; this means that kriging prediction is a linear combination of weighted observations at point p that minimizes prediction variance. Note that this is different (but related) to the Best Linear Unbiased Estimation, or BLUE... although when using kriging to estimate a quantity, the results will be equivalent to the BLUE, which is why kriging is often conceptually understood as equivalent to spatially weighted least squares regression. Below is mathematical formulation for simple kriging, which assumes we are modeling a stationary (i.e., zero mean) process.\n\nWe adopt the standard convention of using s_i for source observations (known), and t_i for target predictions (predicted). Given N observations (s_1…s_n), and a covariance function f_{cov}(), we calculate distances D of N source observations, and use either a function or kernel for source covariances C_{obs}:D(s_1 ... s_n, s_1 ... s_n) = D_{obs}C_{obs} = f_{cov}(D_{obs}) \\quad \\textrm{or} \\quad C(\\mathbf{x}_n, \\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m)\n\nThis is what is happening here:kern = 1.0 * Matern(length_scale=[80000,80000], length_scale_bounds=(1e3, 1e6),\n                        nu=0.5)\ngp1 = GaussianProcessRegressor(kernel=kern, alpha=std_slope**2, optimizer=None).fit(coords_m, mean_slope)\n\nWe are selecting a covariance function (i.e., the Matern kernel), and then fitting that function to our observations on the basis of distance in order to estimate source-to-source covariance, C_{obs}.\n\nUsing the assumption that covariance can now be specified as a function of distance, we can model the covariance between our observations and prediction locations, that is the covariance between the source and target locations C_{obs\\_pred}:D(s_1 ... s_n, t_1 ... t_n) = D_{obs\\_pred}C_{obs\\_pred} = f_{cov}(D_{obs\\_pred})\n\nWhile we don’t actually know the target prediction values of t_1…t_n above, we do know the locations of t_1…t_n, and hence we can calculate their distances from our source s_1 ... s_n locations. This also means we can model the covariance of the predictions t_1…t_n as a function of distance, under the assumption that C_{pred} has the same spatial structure as C_{obs}:D(t_1 ... t_n, t_1 ... t_n) = D_{pred}C_{pred} = f_{cov}(D_{pred})\n\nWe obtain the kriging weights W by linear regression of the source-to-source covariances onto the source-to-target target covariances:W = C_{obs}^{-1} \\cdot C_{obs\\_pred}\n\nAbove we see the key computational constraint for kriging; we need to be able to invert an N by N observation covariance matrix. Kriging is considered spatially weighted least squares because the covariances are calculated as a function of distance, and C_{obs}^{-1} \\cdot C_{obs\\_pred} is Ordinary Least Squares fitting-- which is equivalent to maximum likelihood estimation. The regression/kriging weights are identical for both the observations and their covariances, and we now have enough information to make a kriging/Lstsq’s prediction at the t_1…t_n target locations:[t_1…t_n] =  W^T \\cdot [s_1…s_n]\n\nThe kriging weights are used to produce a prediction at each target location that is a weighted linear combination of our source observation data, which is why the technique is the “Best Linear Unbiased Prediction”-- it is a prediction from a linear combination of observations, with the combination weighting determined by maximum likelihood estimation.\n\nOur ‘mean’ prediction (and the steps above to retrieve them), happens at this call, which also returns the C_{pred}:y_mean, y_cov = gp1.predict(target_c, return_cov=True)\n\nThis gives us the mean of all possible predictions, i.e., y_mean above. If we want to get a prediction instead of the mean of predictions, we use the C_{pred} to draw spatially correlated simulations, which are added to that mean. Specifically, we can preform simulation via Cholesky decomposition* using the derived mean target predictions, and the target-to-target covariance:\n\n(from \n\nsklearn gaussian process module):    def sample_y(self, X, n_samples=1, random_state=0):\n        \"\"\"Draw samples from Gaussian process and evaluate at X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_X, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        n_samples : int, default=1\n            Number of samples drawn from the Gaussian process per query point.\n\n        random_state : int, RandomState instance or None, default=0\n            Determines random number generation to randomly draw samples.\n            Pass an int for reproducible results across multiple function\n            calls.\n            See :term:`Glossary <random_state>`.\n\n        Returns\n        -------\n        y_samples : ndarray of shape (n_samples_X, n_samples), or \\\n            (n_samples_X, n_targets, n_samples)\n            Values of n_samples samples drawn from Gaussian process and\n            evaluated at query points.\n        \"\"\"\n        rng = check_random_state(random_state)\n\n        y_mean, y_cov = self.predict(X, return_cov=True)\n        if y_mean.ndim == 1:\n            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T\n        else:\n            y_samples = [\n                rng.multivariate_normal(\n                    y_mean[:, target], y_cov[..., target], n_samples\n                ).T[:, np.newaxis]\n                for target in range(y_mean.shape[1])\n            ]\n            y_samples = np.hstack(y_samples)\n        return y_samples\n\n*Since we know that our covariance function produces a positive definite gram matrix of covariances, we use cholesky because \n\nit is faster than SVD or eigen decomposition. (The cholesky, SVD, or eigen decomposition is what is called behind the scene for the rng.multivariate_normal function above.)\n\n","type":"content","url":"/gaussianprocesses#what-is-kriging-formally","position":13},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"Why do we care about ‘Prediction’ of slope for glaciology?","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses#why-do-we-care-about-prediction-of-slope-for-glaciology","position":14},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"Why do we care about ‘Prediction’ of slope for glaciology?","lvl2":"Gaussian Processes Prediction"},"content":"Slope is an important variable for glaciologists, but it also doesn’t vary much on the icesheet-- it goes from about 2 degrees at the edge where it’s ‘steep’, before gradually dropping down to 1 in the interior and eventually 0 at the drainage boundary. However, things change when the surface is crevassed; cracks present a range of surface heights and angles, and generally those retrievals are not flat:\n\nCrevasses on the Greenland icesheet aren’t static features, they are linked to hydrologic system, occur seasonally, and can open (or close) on the time scale of hours:\n\nFigure from our \n\n2018 Nature Communications Paper\n\nAlthough we’ve had satellites for Earth Science since the 1970’s starting with the Landsat missions, the record in the arctic is much shorter. The Landsat satellites acquired more data than they had capacity to download, and prioritized downloading data over the United States-- areas over the polar ice sheets and the oceans were deleted until the last decade. We’ve only had consistent remote sensing coverage of the polar icesheets for about a decade, which makes it possible to infer how things are now, but hard to infer trends in how things are changing. Our current \n\nbest observational model of Greenland is Bedmap (\n\nMorlighem, 2017), which a snapshot from 2010 airborne flights modeled over a 150-meter spaced grid. The ICESat mission launched in 2003, has our earliest measurements of elevation and slope across the entire icesheet.\n\nIf we are looking for crevasses, which are often transitory, we can try and infer them from slope ‘anomalies’ that we see in the ICESat data record. We know that the ‘background’ slope of the ice sheet is 3 degrees of slope or lower, and it would be nice to know when and in what areas that slope is exceeded. With simulations of possible slope surfaces, we can count how often 3 degrees of slope is exceeded for a given location:\n\n%%time\nrealizations = rng.multivariate_normal(y_mean, y_cov, size=100, method='cholesky')\n\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  realizations[5]\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=2)\nplt.colorbar()\nplt.show()\n\nThis is one possible ‘realization’ of areas that likely have slopes above 2 degrees, and be crevassed; we can easily look at another:\n\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  realizations[9]\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=2)\nplt.colorbar()\nplt.show()\n\nUltimately, we take all 100 of our simulations, and count when we exceed our slope threshold to get an envelope estimate of where (in the summer of 2006) it was observationally probable to have crevassing:\n\noccurance = np.zeros(5600, dtype=np.int64)\nfor i in realizations:\n    occurance += i > 3\n\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  occurance\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1)) #, norm=mpl.colors.LogNorm())\nticks = np.array([0,20,40,60,80])\nyticks = np.array([100,80,60,40,20])\nplt.xticks(ticks[1:],(ticks[0:-1]*5))\nplt.yticks(yticks,(ticks*5))\nplt.colorbar()\nplt.show()\n\nThere appears to be a crevasse field that’s opened around 150 kilometers inland (around 400 km north of the bottom edge of the drainage basin). This is what the area looked like in early June of 2010 at 300 km north of the basin boundary (red lines denote stress/strain that is high enough for 10 meter deep crevasses to form):\n\nSo stochastic simulations from Gaussian Process modeling suggests crevassing at or above 100 km inland has also happened seasonally in the past north of where we observe it in 2010, with much weaker signal for possible slope anomalies beyond 200 km inland.\n\n","type":"content","url":"/gaussianprocesses#why-do-we-care-about-prediction-of-slope-for-glaciology","position":15},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"Notes on Covariance and Kernels","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses#notes-on-covariance-and-kernels","position":16},{"hierarchy":{"lvl1":"Coding & Writing Sample","lvl3":"Notes on Covariance and Kernels","lvl2":"Gaussian Processes Prediction"},"content":"Several things are apparent for the kriging description above, that highlight both the strengths and weakness of the approach. First and foremost, the approach is dependent on specification of a proper covariance function, or combination of kernels.\n\nObserve the following GP kernels (i.e., covariance models), fitted to the same data; note that every model consistently predicts an additional drop beyond the observations between x=4 and x=5; this is a strength of GPs, since interpolation using most other estimators cannot exceed the min/max of the observation space, even if the data is indicative of a trend!\n\n\n\n\nNote that the next kernel is modeling period functions (applicable to basin and range features)\n\n\n\n\nThe covariance function must be invertible; that is it must be positive definite (Genton, 2002), and the machine that runs the computation must be capable of inverting a N by N matrix. In practice, the N by N inversion can be relaxed if only the ‘best’ prediction is desired (since it can be derived via lstsq calculation); however, simulation and error estimation requires inversion of an N by N matrix. In practice, empirical covariance almost always provides a non-invertible (not positive definite) gram matrix (\n\nHandcock & Stein (1993)); further, a model of covariance is needed to estimate source to target and target to target covariance—so estimation of a covariance function, either via a variogram or by other methods, is always required for geostatistical interpolation and simulation. The error estimation that kriging gives per prediction is error estimation on the assumption that modeled covariance structure is true! Kriging has been characterized as the Best Unbiased Linear Predictor (BULP)… which it is, for a given covariance function. Swapping covariance functions gives different and competing BLUPs, which then need to be evaluated via inter-model comparison (\n\nMacKay (1992)). In short, the kriging error and uncertainty estimation is not the absolute error estimate for the surface, and probability estimation of a prediction must be obtained with other models to give realistic confidence bounds. There are also various flavors of kriging (ordinary, universal, block, etc.) (see \n\nJournel), which are similar to above, and account for non-stationarity.","type":"content","url":"/gaussianprocesses#notes-on-covariance-and-kernels","position":17},{"hierarchy":{"lvl1":"coding-sample"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"coding-sample"},"content":"Sample of coding, writing, and reproducible science communication\n\n\n\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Context for this coding & writing sample"},"type":"lvl1","url":"/gaussianprocesses-copy1","position":0},{"hierarchy":{"lvl1":"Context for this coding & writing sample"},"content":"I presented this notebook on Tuesday, April 29th to Fernando Perez’s research group at UC-Berkely. I’ve modified it slightly to:\n\nAdd context that I had previously noted verbally\n\nReduce the computational complexity of the example so that it can run on a single binder instance\n\nCreated a repository so that software dependencies can be built automatically, and the data is present.\n\nThis notebook can be run for free from any web browser be clicking the following binder link: \n\n\n\n","type":"content","url":"/gaussianprocesses-copy1","position":1},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl2":"Gaussian Processes Prediction"},"type":"lvl2","url":"/gaussianprocesses-copy1#gaussian-processes-prediction","position":2},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl2":"Gaussian Processes Prediction"},"content":"Principle Component Analysis (PCA) and Empirical Orthogonal functions (EOF) are functionally the same numerical method, with EOFs being the preferred term in Atmospheric communities and PCA being the more general term used in broader statistical discussions. Similarly, Gaussian Processes is the general term used in physics and statistical learning for what the geosciences has traditionally termed ‘kriging’, and which some branches of math refer to as Wiener–Kolmogorov prediction.\n\nGaussian Processes (GP) are perhaps best thought of as a collection of enhancements to the traditional kriging system. They are functionally equivalent to kriging (\n\nWilliams (1998)), and differ only in implantation details and slightly in philosophy; although the philosophical differences are in general extensions to ideas explicitly espoused by many kriging giants. The GP philosophy treats interpolation as model selection from an infinite number of functions (\n\nRasmussen & Williams (2005)).\n\nThis characterization certainly fits with \n\nMatheron’s discussion of random fields, and is a close match to \n\nJournal’s description of projection into various solution spaces. These functions are not required to be linear, as the figure below demonstrates:\n\nAs number of predictions ‘P’ grows to be large above, the mean of all ‘P’ approaches the best estimate, with the variance of predictions indicating confidence (std. dev. of predictions)\n\nThe primary object of interest in kriging is the variogram, which is the main method by which practitioners fit a covariance function. In contrast, the primary object of interest in GPs is the kernel, which is directly equivalent to covariances. The kernel formulation has direct computational savings, enabling solving of high dimensional kernel functions in low dimensional space via the kernel trick (i.e., computing only the inner products) (\n\nGenton (2002)); the kernel formulation also admits easy translation to Fourier based methods when interpolating regular spaces (\n\nWilson & Adams (2013)). Kernel architecture is flexible, as valid kernels can be added or multiplied to form new synthesis kernels (\n\nCressie & Huang (1999)). For instance, non-stationary kernels can be combined with stationary kernels and white noise kernels, such that large spatial trends, local correlation and structure, and intrinsic random noise and error can be modeled and specified as discrete submodules and functions-- \n\nsee here for a practical example using Mauna Loa data in scikit-learn. Kernels can also be bounded for more efficient computation—for instance, circular and spherical kernels give rise to sparse gram matrices that can be reordered and solved by efficient sparse solvers; the Matérn kernel (\n\nMatérn, 1960) can be modified to do the same. Separable non-stationary kernels can be written as a Kronecker tensor product (\n\nGenton (2002)), and solved in circulant matrixes by efficient Fourier methods (\n\nWilson & Nickisch (2015)).\n\nAlthough Gaussian Processes is the more general statistical term for covariance methods of prediction, the practice and term of kriging predates Gaussian Processes. Thus, to explain the history and theory of Gaussian Processes, we’ll start with kriging first.\n\n","type":"content","url":"/gaussianprocesses-copy1#gaussian-processes-prediction","position":3},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"What is kriging (informally)?","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses-copy1#what-is-kriging-informally","position":4},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"What is kriging (informally)?","lvl2":"Gaussian Processes Prediction"},"content":"Generally speaking, most people use kriging as an interpolator. Classic interpolators can be split into ‘estimators’ and ‘predictors’; this distinction (in line with \n\nCressie, 1990) mainly applies to highlight that ‘estimators’ provide an estimate of the surface, while ‘predictors’ assign a prediction that is accompanied by measures of confidence (probability, confidence interval).\n\nEstimators including nearest neighbor, bilinear interpolation, and cubic spline (bottom), as compared with kriging predictions using three different kernels (top)\n\nTraditional ‘estimators’ are of the fairly boring type that most are fairly familiar with— canonical examples are bilinear interpolation, Inverse Distance Weighting (IDW), cubic convolution, etc. Other non-traditional examples of ‘estimators’ include things like nearest and natural neighbor interpolation, which are appropriate for specific domain cases when you want to preserve original data values. If your goal is just interpolation and you have sufficient data, using an estimator is perfectly fine-- and fast! However, if your data is sparse, kriging starts to become more appealing:\n\nThis sparse estimation capability is why kriging was originally developed. Kriging is named after Danie G. Krige, who’s 1951 master’s thesis developed and described what we call ‘Ordinary Kriging’. Danie Krige was a prospector looking for gold in South Africa, an application that had sparse input measurements, and expensive sampling. Krige’s method was coined ‘kriging’ by the early 1960’s by French mathematician Georges Matheron, who formalized and expanded the method.\n\nOf course, the other case where kriging is appealing is when you need to use the method as a predictor.\n\n","type":"content","url":"/gaussianprocesses-copy1#what-is-kriging-informally","position":5},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"Covariance, Gold, and the variogram model","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses-copy1#covariance-gold-and-the-variogram-model","position":6},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"Covariance, Gold, and the variogram model","lvl2":"Gaussian Processes Prediction"},"content":"\n\nTraditional kriging defines a model of covariance by fitting a function to the empirical ‘variogram’, which plots the variance between observations as a function of distance between those observations. The variogram ‘sill’ refers to variance between uncorrelated samples, and the ‘range’ is the lag-distance at which this paired observation decorrelation occurs. The ‘nugget’ refers to the intrinsic variance (i.e., the observational uncertainty) at distance zero-- that is, the variance of a single point observation with itself. The term ‘nugget’ is literally referring to ‘gold nugget’ in the context of prospecting; i.e., finding a ‘nugget’ of gold which has been displaced from the source gold deposit... just because a ‘nugget’ is present at a location, does not guarantee that you are coincident with the deposit that generated that observation; however, the expectation is that the nugget is close!\n\nMathematically, specifying a ‘nugget’ value (or alpha in GP terminology) is effectively adding a constant to the diagonal of the covariance matrix such that each observation has some level of variance with itself. Doing this allows flexibility for the mean of the predictions to not intersect all of the observation points-- if covariance is set to zero along the diagonal, then any estimation will be a ‘rubber sheeting’ that ensures the output surface prediction passes through the original data points. Of course, we can also specify covariance per observation rather than as a constant along the diagonal, and selectively down weight low confidence observations.\n\n","type":"content","url":"/gaussianprocesses-copy1#covariance-gold-and-the-variogram-model","position":7},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"What I use Gaussian Processes for","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses-copy1#what-i-use-gaussian-processes-for","position":8},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"What I use Gaussian Processes for","lvl2":"Gaussian Processes Prediction"},"content":"\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom osgeo import osr\nimport pyproj\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel, RBF\n\nrng = np.random.default_rng() # numpy sampling API\n#np.random.seed(1234)\n#np.random.seed(55)\nnp.random.seed(123)\n\ndata = pd.HDFStore(\"./Z62SU06\" + \".h5\", 'r') # ICESat data\ncoordlist = data['coords'] # Needed for distance calculations / plotting\ndf_indices = data['indices'] # UUID to link to observations to slope retrievals\ndf_corr = data['corr'] # Filter and weight data\n\ngslope = np.load('./General_slopes.npy') # Sub ICESat footprint slope retrievals\nmean_slope = np.zeros(len(coordlist))\nstd_slope = np.zeros(len(coordlist))\n\n# Merge and aggregate slope\nfor i in tqdm(range(len(coordlist))):\n    sweights = np.array(df_corr.iloc[i].values)\n    weightidx = np.isfinite(sweights)\n    sweights = np.abs(sweights[weightidx])\n    sidx = df_indices.iloc[i].values[weightidx]\n    sidx = np.array(sidx, dtype=int)\n    mean_slope[i] = np.average(gslope[sidx], weights=sweights)\n    std_slope[i] = np.std(gslope[sidx])\n\nThis example is meant to be larger than a ‘toy’ dataset, but still small enough to interact with and demonstrate. Our input data consist of 23,359 observations (from the summer of 2006), which are within footprint slope retrievals from the first ICESat mission (which flew from 2003 to 2009, although laser power was strongest in the first three years).\n\nHere’s what the data looks like:\n\nfig, ax = plt.subplots()\nim = ax.scatter(coordlist.lon, coordlist.lat, c=mean_slope, s=std_slope**2)\nim.set_clim(min(mean_slope), max(mean_slope))\nfig.colorbar(im, ax=ax, label=\"Degrees of slope \\n (points scaled by retrieval variance)\")\nplt.show()\n\nThis is an interesting case of geostatistics because the sampling is quite odd-- the along track resolution is dense, but the across track resolution is sparse. Most of the code is actually setting up the source and target coordinate systems for the model. The input observational data is irregularly spaced and in lat/lon coordinates, but we want our distance measurements to be in euclidean space and output to a grid.\n\nWe also want to ‘clip’ the results to a drainage basin. This is for two reasons-- first, the drainage basin boundries are of zero surface slope and provide a physically meaningful way to bound, divide, and tile the analysis. And second, covariance functions are expensive, so clipping and masking the edges of our area of interest helps things run faster... it also makes the output look good!\n\n# Projection object\nds = osr.SpatialReference()\nds.ImportFromEPSG(3411) # polar projection\nds.ExportToProj4()\np2 = pyproj.Proj(ds.ExportToProj4())\n\n# Observations to meters from lat / lon\nX, Y = p2(np.array(coordlist.lon),\n          np.array(coordlist.lat))\n# To vector form sklearn\ncoords_m = np.ones((len(coordlist),2))\ncoords_m[:,0] = X\ncoords_m[:,1] = Y\n\n# Drainage basin boundries\nzwally = pd.read_csv('./GrnDrainageSystems_Ekholm.txt',\n                     sep=r\"\\s+\", names=['basin', 'lat','long'])\nbasin = '6.2'\nLL = zip(zwally[zwally.basin == float(basin)].long,\n         zwally[zwally.basin == float(basin)].lat)\nLL = list(LL)\n# Convert basin boundries to meters from lat / lon\npX, pY = p2(np.array(LL)[:,0],\n            np.array(LL)[:,1])\n# Polygon object for masking\nZ = mpl.path.Path(list(zip(pX, pY)))\n\n# Setting up kriging grid\nx1, y1 = np.meshgrid(np.r_[round(min(pX), -2) - 2500:round(max(pX), -2) + 2500:5000],\n                     np.r_[round(min(pY), -2) - 2500:round(max(pY), -2) + 2500:5000])\nkcoords = np.vstack([x1.ravel(),y1.ravel()]).T\n\n# Masking kriging grid\nZidx = Z.contains_points(kcoords[:])\ntarget_c = kcoords[Zidx] # Target coordinates\nx1.shape, len(target_c)\n\nAt this point, we’ve setup a 2-km grid to predict on that’s 267 by 273 (a little over 500km per side), which after masking has 34,984 target coordinate locations to predict at. The actual Gaussian Process model specification is fairly short:\n\nind = np.array(range(0,len(coordlist)), dtype=int)\nidxs = np.random.choice(ind, size=4000)\n\n%%time\nnoise = WhiteKernel(noise_level=.5)\nrbf = 3 * RBF(length_scale=[80000,80000])\nkern = 2.0 * Matern(length_scale=[80000,80000], length_scale_bounds=(1e3, 1e6),\n                        nu=0.5)\ngp1 = GaussianProcessRegressor(kernel=rbf+kern+noise, alpha=std_slope[idxs]**2, optimizer=None).fit(coords_m[idxs], mean_slope[idxs])\n\nWe are specifying a Matern kernel to calculate and estimate our covarience, which is defined by the single parameter nu; this parameter controls how smooth the fuctions are within our prior distribution of possible surface functions. It is set at 0.5 (i.e., ‘1 / 2’, equivelent the absolute exponential kernel) to allow rough, non-differentiable candidate functions; a nu=1.5 (3 / 2) will select from functions that are once differentiable, nu=2.5 (5 / 2) will select from functions that are twice differentiable, and nu=inf will converge to the infinately differentiable RBF kernel.\n\nThe length_scale parameter would typically be optimized during model fit based on the spatial structure of the data... however, because our input data is heavily unbalanced (i.e., dense in the along track direction, sparse in the across track), hyper-parameter optimization sets a length scale that is too short, so it is fixed at 80km, with similar settings applied for the length_scale_bounds.\n\nAfter model specification, at fit time we provide the coordinates, and slope values, along with an alpha that is set to varience of slope retrieval (i.e., estimates of varience per observation).\n\nDoubling the number of observations (by adding a second summer of orbits) pushes the model fit time from under two minutes to about a half hour (although some of this is due to the higher memory footprint and pressure causing the OS to begin swapping to disk).\n\nWe can get the mean surface estimate and target-to-target covariances with the following:\n\n%%time\ny_mean, y_cov = gp1.predict(target_c, return_cov=True)\n\nThe mean predictions need to be reshaped before plotting:\n\n# slope\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  y_mean\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=0, vmax=10)\nplt.colorbar()\nplt.show()\n\n","type":"content","url":"/gaussianprocesses-copy1#what-i-use-gaussian-processes-for","position":9},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"What is kriging (formally)?","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses-copy1#what-is-kriging-formally","position":10},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"What is kriging (formally)?","lvl2":"Gaussian Processes Prediction"},"content":"Mathematically, kriging is the Best Unbiased Linear Predictor (\n\nJournel (1977) ; \n\nCressie (1990) ; \n\nHandcock & Stein (1993)), and provides the Best Linear Unbiased Prediction, or BLUP; this means that kriging prediction is a linear combination of weighted observations at point p that minimizes prediction variance. Note that this is different (but related) to the Best Linear Unbiased Estimation, or BLUE... although when using kriging to estimate a quantity, the results will be equivilent to the BLUE, which is why kriging is often conceptually understood as equivalent to spatially weighted generalized least squares regression. There are of course various flavors of kriging (simple, ordinary, universal, etc.) (see \n\nJournel), but below is mathematical formulation for simple kriging (other flavors are similar):\n\nGiven N observations (s1…sn), and CovF(distance) covariance function, calculate D distances of N observations; note the convention of using t for target and s for source:D(s_1 ... s_n, s_1 ... s_n) = D_{obs}C_{obs} = f_{cov}(D_{obs}) \\quad \\textrm{or} \\quad C(\\mathbf{x}_n, \\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m)Dist(s1…sn, s1…sn) = Dobs  \t\t# square distance matrix\nCobs = CovF(Dobs)                       # observation covarience, i.e., source-to-source cov\n\nThis is what is happening here:kern = 1.0 * Matern(length_scale=[80000,80000], length_scale_bounds=(1e3, 1e6),\n                        nu=0.5)\ngp1 = GaussianProcessRegressor(kernel=kern, alpha=std_slope**2, optimizer=None).fit(coords_m, mean_slope)\n\nWe are selecting a covariance function (the Matern kernel), and then fitting that fuction to our observations on the basis of distance in order to estimate source-to-source cov\n\nDefine p1…pn prediction nodes; calculate distances and Cpred (covariance of predictions):D(t_1 ... t_n, t_1 ... t_n) = D_{pred}C_{pred} = f_{cov}(D_{pred})Dist(t1…tn, t1…tn) = Dpred  \t\t# square distance matrix\nCpred = CovF(Dpred)                     # target covarience, i.e., target-to-target cov\n\nNote that for above, we don’t actually know p1…pn but we do know the locations of p1…pn, and hence we can calculate their distances, and given that our covariance function is a function of distances, we can model the covariance of the predictions p1…pn as a function of distance.\n\nAs mentioned above, kriging is equivalent to spatial least squares regression. Hence, we can retrieve our best estimate at locations p1…pn though least square regression—see below for useful equivalencies:Lstsq( s1…sn, t1…tn) == Lstsq( Cobs, Cobs_pred) == kriging_wieghts\n\n…where Cobs_pred is the covariance between our observations and predictions, i.e., our target-to-source cov. Of course, we don’t know our predictions yet; all we have are our observations, and our observation covariances. However, we have defined a covariance function, based on our observation covariances, that provides covariance as a function of distance; hence above we defined our prediction to prediction covariance.\n\nThe kriging weights are used to weight a combination of our observations at each prediction location-- with the weights being determined by distance and the covariance structure. Our ‘mean’ prediction and target-to-target cov (and the steps above and below to retrieve them), happen at this call:y_mean, y_cov = gp1.predict(target_c, return_cov=True)\n\n(Note, this call is also implicitly calculating the source to target covariance!)\n\nUsing the same assumption that covariance can be specified as a function of distance, we can get an estimate of covariance between our observations and prediction locations:Cobs_pred = CovF(Dist(s1…sn, t1…tn)\n\n…now we obtain the kriging weights by:kriging_wieghts = dot(inv(Cobs), Cobs_pred)\n\nAbove we see the key computational constraint for kriging; we need to be able to invert an N by N observation covariance matrix.\n\nGiven that the regression/kriging weights are identical for both the observations and their covariances, we now have enough information to make a kriging/Lstsq’s prediction of t1…tn:t1…tn =  dot(kriging_wieghts.T, s1…sn)\n\nWe can calculate partial correlation/covariance, which is equivalent to the covariance of the residual error from kriging/regression. We get conditional covariance with:Cpred – dot(dot(Cobs_pred.T, inv(Cobs)), Cobs_pred)\n\nThe above gives the estimation error covariance, and allows for simulation, i.e., prediction. Specifically, we can preform simulation via Cholesky decomposition* using the derived mean target predictions, and the target-to-target covariance:\n\n(from \n\nsklearn gaussian process module):    def sample_y(self, X, n_samples=1, random_state=0):\n        \"\"\"Draw samples from Gaussian process and evaluate at X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_X, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        n_samples : int, default=1\n            Number of samples drawn from the Gaussian process per query point.\n\n        random_state : int, RandomState instance or None, default=0\n            Determines random number generation to randomly draw samples.\n            Pass an int for reproducible results across multiple function\n            calls.\n            See :term:`Glossary <random_state>`.\n\n        Returns\n        -------\n        y_samples : ndarray of shape (n_samples_X, n_samples), or \\\n            (n_samples_X, n_targets, n_samples)\n            Values of n_samples samples drawn from Gaussian process and\n            evaluated at query points.\n        \"\"\"\n        rng = check_random_state(random_state)\n\n        y_mean, y_cov = self.predict(X, return_cov=True)\n        if y_mean.ndim == 1:\n            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T\n        else:\n            y_samples = [\n                rng.multivariate_normal(\n                    y_mean[:, target], y_cov[..., target], n_samples\n                ).T[:, np.newaxis]\n                for target in range(y_mean.shape[1])\n            ]\n            y_samples = np.hstack(y_samples)\n        return y_samples\n\nSeveral things are apparent for the kriging description above, that highlight both the strengths and weakness of the approach. First and foremost, the approach is completely dependent on specification of a proper covariance function. The covariance function must be invertible; that is it must be positive definite (Genton, 2002), and the machine that runs the computation must be capable of inverting a N by N matrix. In practice, the N by N inversion can be relaxed if only the ‘best’ prediction is desired (since it can be derived via lstsq calculation); however, simulation and error estimation requires inversion of an N by N matrix. In practice, empirical covariance almost always provides a non-invertible (not positive definite) gram matrix (\n\nHandcock & Stein (1993)); further, a model of covariance is needed to estimate source to target and target to target covariance—so estimation of a covariance function, either via a variogram or by other methods, is always required for geostatistical interpolation and simulation. The error estimation that kriging gives per prediction is error estimation on the assumption that modeled covariance structure is true! Kriging has been characterized as the Best Unbiased Linear Predictor (BULP)… which it is, for a given covariance function. Swapping covariance functions gives different and competing BLUPs, which then need to be evaluated via inter-model comparison (\n\nMacKay (1992)). In short, the kriging error and uncertainty estimation is not the absolute error estimate for the surface, and probability estimation of a prediction must be obtained with other models to give realistic confidence bounds.\n\n*Traditional kriging involves estimation of a variogram, however, the variogram is simply a way of producing a covariance function that produces a positive definite gram matrix of covariances (Genton, 2002). Since we know that our covariance function produces a positive definite gram matrix of covariances, we use cholesky because \n\nit is faster than SVD or eigen decomposition.\n\n","type":"content","url":"/gaussianprocesses-copy1#what-is-kriging-formally","position":11},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"Why Prediction?","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses-copy1#why-prediction","position":12},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"Why Prediction?","lvl2":"Gaussian Processes Prediction"},"content":"Slope is an important variable for glaciologists, but it also doesn’t vary much on the icesheet-- it goes from about 2 degrees at the edge where it’s ‘steep’, before gradually dropping down to 1 in the interior and eventually 0 at the drainage boundary. However, things change when the surface is crevassed; cracks present a range of surface heights and angles, and generally those retrievals are not flat.\n\nSo if we are looking for crevasses, and we know that the ‘background’ slope of the ice sheet is 2 degrees of slope or lower, it would be nice to know when that slope is exceeded. Unfortunately, our mean prediction of surface slope is smoothing the variation in slope retrievals-- if we have a distribution of slope values, the best prediction will trend towards the mean, resulting in surface that suppresses variation we know is present.\n\nWhat we can do though, is simulate possible surfaces, and count how often we exceed 2 degrees of slope for a given location:\n\n%%time\nrealizations = rng.multivariate_normal(y_mean, y_cov, size=100, method='cholesky')\n\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  realizations[5]\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=2)\nplt.colorbar()\nplt.show()\n\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  realizations[9]\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=2)\nplt.colorbar()\nplt.show()\n\noccurance = np.zeros(5600, dtype=np.int64)\nfor i in realizations:\n    occurance += i > 3\n\n\n\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  occurance\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1)) #, norm=mpl.colors.LogNorm())\nticks = np.array([0,20,40,60,80])\nyticks = np.array([100,80,60,40,20])\nplt.xticks(ticks[1:],(ticks[0:-1]*5))\nplt.yticks(yticks,(ticks*5))\nplt.colorbar()\nplt.show()\n\n","type":"content","url":"/gaussianprocesses-copy1#why-prediction","position":13},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"Covarience Kernels","lvl2":"Gaussian Processes Prediction"},"type":"lvl3","url":"/gaussianprocesses-copy1#covarience-kernels","position":14},{"hierarchy":{"lvl1":"Context for this coding & writing sample","lvl3":"Covarience Kernels","lvl2":"Gaussian Processes Prediction"},"content":"Observe the following GP kernels (i.e., covarience models), fitted to the same data; note that every model consistently predicts an additional drop beyond the observations between x=4 and x=5; interpolation using most estimators cannot exceed the min/max of the observation space!\n\n\n\n\nNote that the next kernel is modeling period functions (applicable to basin and range features)\n\n\n\n\nNote that one major benefit to the GP kernel formulation is that \n\nkernels can be combined. This approach models kernels to account for the non-stationarity of the processes within the kernel formulation, rather than seperating out a","type":"content","url":"/gaussianprocesses-copy1#covarience-kernels","position":15}]}