{"version":2,"kind":"Notebook","sha256":"53a30412fd66e629c106382baa7d0c0db3e65b1eb251a8a9b0d3d5b98dd59b8c","slug":"gaussianprocesses-copy1","location":"/tmp/GaussianProcesses-Copy1.ipynb","dependencies":[],"frontmatter":{"title":"Context for this coding & writing sample","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"numbering":{"title":{"offset":1}},"thumbnail":"/build/b77199e99a54e59b2e3c037c2cc90f21.svg","exports":[{"format":"ipynb","filename":"GaussianProcesses-Copy1.ipynb","url":"/build/GaussianProcesses-Co-7c0109518ad71989a5d6ace27a9debb8.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"I presented this notebook on Tuesday, April 29th to Fernando Perez’s research group at UC-Berkely. I’ve modified it slightly to:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"mz4aFM2HFx"}],"key":"D6H0hZtuwP"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Add context that I had previously noted verbally","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"g8g2DON040"}],"key":"ZyIVBO5hTl"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Reduce the computational complexity of the example so that it can run on a single binder instance","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Y2ue1BoyKN"}],"key":"V61GbQnWYx"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Created a repository so that software dependencies can be built automatically, and the data is present.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"JDYS58ggtO"}],"key":"fnsAX7pjfR"}],"key":"j1IdIm1H5q"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"This notebook can be run for free from any web browser be clicking the following binder link: ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"gkcNVDLCZj"},{"type":"link","url":"https://mybinder.org/v2/gh/espg/coding-sample/HEAD?urlpath=%2Fdoc%2Ftree%2FGaussianProcesses.ipynb","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"image","url":"/build/b77199e99a54e59b2e3c037c2cc90f21.svg","alt":"Binder","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"VmQkDJybFE","urlSource":"https://mybinder.org/badge_logo.svg"}],"urlSource":"https://mybinder.org/v2/gh/espg/coding-sample/HEAD?urlpath=%2Fdoc%2Ftree%2FGaussianProcesses.ipynb","key":"RFXHjvivjO"}],"key":"Ll9X7VCHNq"}],"key":"dZcS9NkH1f"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Gaussian Processes Prediction","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tuOlo1QEzN"}],"identifier":"gaussian-processes-prediction","label":"Gaussian Processes Prediction","html_id":"gaussian-processes-prediction","implicit":true,"key":"ctzUIYPlzJ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Principle Component Analysis (PCA) and Empirical Orthogonal functions (EOF) are functionally the same numerical method, with EOFs being the preferred term in Atmospheric communities and PCA being the more general term used in broader statistical discussions. Similarly, Gaussian Processes is the general term used in physics and statistical learning for what the geosciences has traditionally termed ‘kriging’, and which some branches of math refer to as Wiener–Kolmogorov prediction.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TmsuV1seMp"}],"key":"SggigGdNjD"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Gaussian Processes (GP) are perhaps best thought of as a collection of enhancements to the traditional kriging system. They are functionally equivalent to kriging (","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SIV9HIjJRR"},{"type":"cite","url":"https://doi.org/10.1007/978-94-011-5014-9_23","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Williams (1998)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ox1CRda2YZ"}],"kind":"narrative","label":"Williams_1998","identifier":"https://doi.org/10.1007/978-94-011-5014-9_23","enumerator":"1","key":"MvcxXpdvxo"},{"type":"text","value":"), and differ only in implantation details and slightly in philosophy; although the philosophical differences are in general extensions to ideas explicitly espoused by many kriging giants. ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"nX5zGnAb4X"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The GP philosophy treats interpolation as model selection from an infinite number of functions (","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"obRumKEpDr"},{"type":"cite","url":"https://doi.org/10.7551/mitpress/3206.001.0001","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Rasmussen & Williams (2005)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"W40QuIzMXQ"}],"kind":"narrative","label":"Rasmussen_2005","identifier":"https://doi.org/10.7551/mitpress/3206.001.0001","enumerator":"2","key":"B8WE1gT0yX"},{"type":"text","value":")","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"PJ0xOW4n3e"}],"key":"jgRP4JAAjJ"},{"type":"text","value":".","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"hkl9d5JZoG"}],"key":"ubE0Mxmxvx"},{"type":"image","url":"/build/5b8ae75218c2b71222b94cc7a0abdc35.png","alt":"gp.png","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"IhcbhHl742","urlSource":"https://upload.wikimedia.org/wikipedia/commons/7/7f/Gaussian_Process_Regression.png","urlOptimized":"/build/5b8ae75218c2b71222b94cc7a0abdc35.webp"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"This characterization certainly fits with ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ysOkZ4lpNZ"},{"type":"cite","url":"https://doi.org/10.2307/1425829","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Matheron’s","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ipu2MllyfR"}],"kind":"narrative","label":"Matheron_1973","identifier":"https://doi.org/10.2307/1425829","enumerator":"3","key":"kSKs2XIQym"},{"type":"text","value":" discussion of random fields, and is a close match to ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Lii1UzD5PT"},{"type":"cite","url":"https://doi.org/10.1007/bf02067214","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Journal’s","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"mcKbEHxowA"}],"kind":"narrative","label":"Journel_1977","identifier":"https://doi.org/10.1007/bf02067214","enumerator":"4","key":"tdQWbMu2T8"},{"type":"text","value":" description of projection into various solution spaces. These functions are not required to be linear, as the figure below demonstrates:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"AcAPp75pM4"}],"key":"sUzvcvbw5h"},{"type":"image","url":"/build/af20bbeeb575a136bc4a46923a3b5451.gif","alt":"animation.gif","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"JmcDGugKw5","urlSource":"https://upload.wikimedia.org/wikipedia/commons/d/da/Gaussianprocess.gif","urlOptimized":"/build/af20bbeeb575a136bc4a46923a3b5451.webp"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"As number of predictions ‘P’ grows to be large above, the mean of all ‘P’ approaches the best estimate, with the variance of predictions indicating confidence (std. dev. of predictions)","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"O4Cp1vtnIi"}],"key":"unaBS4x179"}],"key":"MIxebkg2rI"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"strong","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"The primary object of interest in kriging is the variogram","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"NdaYmUX6jg"}],"key":"adA3ond8nK"},{"type":"text","value":", which is the main method by which practitioners fit a covariance function. ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Y2loZz3MlP"},{"type":"strong","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"In contrast, the primary object of interest in GPs is the kernel, which is directly equivalent to covariances.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Q6TM6VkLpa"}],"key":"NHaB0Sshte"},{"type":"text","value":" The kernel formulation has direct computational savings, enabling solving of high dimensional kernel functions in low dimensional space via the kernel trick (i.e., computing only the inner products) (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"VjcVJSy7KW"},{"type":"cite","identifier":"genton","label":"genton","kind":"narrative","position":{"start":{"line":15,"column":455},"end":{"line":15,"column":462}},"children":[{"type":"text","value":"Genton (2002)","key":"XPp0YAVa3L"}],"enumerator":"5","key":"YxPwtvfi7R"},{"type":"text","value":"); the kernel formulation also admits easy translation to Fourier based methods when interpolating regular spaces (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"PmfIv9HE5E"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.1302.4245","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Wilson & Adams (2013)","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"W49fevm0Ie"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1302.4245","identifier":"https://doi.org/10.48550/arXiv.1302.4245","enumerator":"6","key":"fxxl4nA0iY"},{"type":"text","value":"). Kernel architecture is flexible, as valid kernels can be added or multiplied to form new synthesis kernels (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"GoVCoxBh8h"},{"type":"cite","url":"https://doi.org/10.1080/01621459.1999.10473885","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Cressie & Huang (1999)","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"ffi8CK1Zqs"}],"kind":"narrative","label":"Cressie_1999","identifier":"https://doi.org/10.1080/01621459.1999.10473885","enumerator":"7","key":"RlMu7zsozB"},{"type":"text","value":"). For instance, non-stationary kernels can be combined with stationary kernels and white noise kernels, such that large spatial trends, local correlation and structure, and intrinsic random noise and error can be modeled and specified as discrete submodules and functions-- ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"drc1EFSu0n"},{"type":"link","url":"https://towardsdatascience.com/gaussian-process-kernels-96bafb4dd63e/","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"see here for a practical example using Mauna Loa data","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"DAAVOtGR10"}],"urlSource":"https://towardsdatascience.com/gaussian-process-kernels-96bafb4dd63e/","key":"D7xVvxUtmm"},{"type":"text","value":" in scikit-learn. Kernels can also be bounded for more efficient computation—for instance, circular and spherical kernels give rise to sparse gram matrices that can be reordered and solved by efficient sparse solvers; the Matérn kernel (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"NfLsgwRN2z"},{"type":"cite","url":"https://doi.org/10.1007/978-1-4615-7892-5","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Matérn, 1960","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"G2BMRyCeFk"}],"kind":"narrative","label":"Mat_rn_1986","identifier":"https://doi.org/10.1007/978-1-4615-7892-5","enumerator":"8","key":"Kv2rQFEdXJ"},{"type":"text","value":") can be modified to do the same. Separable non-stationary kernels can be written as a Kronecker tensor product (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"IP9jTLLDiI"},{"type":"cite","identifier":"genton","label":"genton","kind":"narrative","position":{"start":{"line":15,"column":1635},"end":{"line":15,"column":1642}},"children":[{"type":"text","value":"Genton (2002)","key":"wwPMf7WXBa"}],"enumerator":"5","key":"I6Ol7cKmHe"},{"type":"text","value":"), and solved in circulant matrixes by efficient Fourier methods (","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"fqam3hS1RE"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.1503.01057","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Wilson & Nickisch (2015)","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"gAhLtceEUP"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1503.01057","identifier":"https://doi.org/10.48550/arXiv.1503.01057","enumerator":"9","key":"eys8ShWcvH"},{"type":"text","value":").","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"P0mp2QpgDp"}],"key":"oo0pG1vuPD"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Although Gaussian Processes is the more general statistical term for covariance methods of prediction, the practice and term of kriging predates Gaussian Processes. Thus, to explain the history and theory of Gaussian Processes, we’ll start with kriging first.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"mAUMxWeei9"}],"key":"LO5zKfibZS"}],"key":"IxG2Tr8wEq"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What is kriging (informally)?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jlRTVlLFrs"}],"identifier":"what-is-kriging-informally","label":"What is kriging (informally)?","html_id":"what-is-kriging-informally","implicit":true,"key":"pjq4ewPNrV"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Generally speaking, most people use kriging as an interpolator. Classic interpolators can be split into ‘","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TUFeBXHyEc"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"estimators","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"i8C2qUKzen"}],"key":"efkyytlPHQ"},{"type":"text","value":"’ and ‘","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"unggRfg56h"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"predictors","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vL0mdFw9Po"}],"key":"EaIgLLmVf4"},{"type":"text","value":"’; this distinction (in line with ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"MDbfDn7gXd"},{"type":"cite","url":"https://doi.org/10.1007/bf00889887","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Cressie, 1990","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"iXDXzh4rIt"}],"kind":"narrative","label":"Cressie_1990","identifier":"https://doi.org/10.1007/bf00889887","enumerator":"10","key":"FHkSZkWGph"},{"type":"text","value":") mainly applies to highlight that ‘","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pWoDfqsmuu"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"estimators","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"j17AuerTn4"}],"key":"pdRjd1NXK4"},{"type":"text","value":"’ provide an estimate of the surface, while ‘","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"brpgrMfkAI"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"predictors","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"b97ifCFRmc"}],"key":"BfcgbHI0GK"},{"type":"text","value":"’ assign a prediction that is accompanied by measures of confidence (probability, confidence interval).","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hOAis7Sfic"}],"key":"I6xdCrkzF9"},{"type":"image","url":"/build/03d1076e4e32f4695734b1167c5c65c6.jpeg","alt":"image.jpg","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ACtPvxnC48","urlSource":"https://github.com/YuePanEdward/PPE_Kriging/blob/main/figures/result_dense_dataset.jpg?raw=true","urlOptimized":"/build/03d1076e4e32f4695734b1167c5c65c6.webp"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Estimators including nearest neighbor, bilinear interpolation, and cubic spline (bottom), as compared with kriging predictions using three different kernels (top)","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PkORrdNzwS"}],"key":"KhJuWkbbeg"}],"key":"l0jpwVBPrp"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Traditional ‘","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"GVfbl5Vb3A"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"estimators","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"lR20sXFWYD"}],"key":"l6OubFpNmh"},{"type":"text","value":"’ are of the fairly boring type that most are fairly familiar with— canonical examples are bilinear interpolation, Inverse Distance Weighting (IDW), cubic convolution, etc. Other non-traditional examples of ‘estimators’ include things like nearest and natural neighbor interpolation, which are appropriate for specific domain cases when you want to preserve original data values. If your ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"KcvB76OquT"},{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"goal","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"YILudF26Tf"}],"key":"q7YTngpnRN"},{"type":"text","value":" is ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"bTaZ50k0b8"},{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"just","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"vkk0c3GsGr"}],"key":"vzoHjfEp7r"},{"type":"text","value":" interpolation ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"gkzXT4BvB5"},{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"and you have sufficient data","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"EcX8XcOmIR"}],"key":"HVc04c6IyC"},{"type":"text","value":", using an estimator is perfectly fine-- and fast! However, if your data is ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"qrMvUhrRMI"},{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"sparse","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"pNqbSDssab"}],"key":"FNWfTT0ef0"},{"type":"text","value":", kriging starts to become more appealing:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ECXrGJRpj3"}],"key":"rGFIlSKx4F"},{"type":"image","url":"/build/bdb989a18e2c78b76f49ebabc3b22a39.jpeg","alt":"image.jpg","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"WXPAo6CO42","urlSource":"https://github.com/YuePanEdward/PPE_Kriging/blob/main/figures/result_sparse_dataset.jpg?raw=true","urlOptimized":"/build/bdb989a18e2c78b76f49ebabc3b22a39.webp"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"This sparse estimation capability is why kriging was originally developed. Kriging is named after Danie G. Krige, who’s 1951 master’s thesis developed and described what we call ‘Ordinary Kriging’. Danie Krige was a prospector looking for gold in South Africa, an application that had sparse input measurements, and expensive sampling. Krige’s method was coined ‘kriging’ by the early 1960’s by French mathematician Georges Matheron, who formalized and expanded the method.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"v7zWabYLjC"}],"key":"Ej3QXbWVuV"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Of course, the other case where kriging is appealing is when you need to use the method as a ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"lDGRVICgmQ"},{"type":"emphasis","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"predictor","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"frTnEgbyAi"}],"key":"D1IQ05OvtJ"},{"type":"text","value":".","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"U8VtC2HZ68"}],"key":"ySdIjdzPJ8"}],"key":"pP3uq29nGv"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Covariance, Gold, and the variogram model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gi94sCWDMc"}],"identifier":"covariance-gold-and-the-variogram-model","label":"Covariance, Gold, and the variogram model","html_id":"covariance-gold-and-the-variogram-model","implicit":true,"key":"ziMjdweEKn"},{"type":"image","url":"/build/2ffed4862480bba556962ff294be9d78.gif","alt":"image.gif","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LIPhCTHJUo","urlSource":"https://vsp.pnnl.gov/help/image/Variogram.gif","urlOptimized":"/build/2ffed4862480bba556962ff294be9d78.webp"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Traditional kriging defines a ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"zvCsvjHEtW"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"model of covariance","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"x5iqQAIwqD"}],"key":"giiBbqtARB"},{"type":"text","value":" by fitting a function to the empirical ‘variogram’, which plots the variance between observations as a function of distance between those observations. The variogram ‘sill’ refers to variance between uncorrelated samples, and the ‘range’ is the lag-distance at which this paired observation decorrelation occurs. The ‘nugget’ refers to the intrinsic variance (i.e., the observational uncertainty) at distance zero-- that is, the variance of a single point observation with itself. The term ‘nugget’ is literally referring to ‘gold nugget’ in the context of prospecting; i.e., finding a ‘nugget’ of gold which has been displaced from the source gold deposit... just because a ‘nugget’ is present at a location, does not guarantee that you are coincident with the deposit ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AxYGnonOgi"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"that generated that observation","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"wdT22AP5Lc"}],"key":"yIc93VX6L2"},{"type":"text","value":"; however, the expectation is that the nugget is close!","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"lPmYLTtaEp"}],"key":"w7JsyRtzVD"},{"type":"image","url":"/build/ff8c410b251590af858bbaf80f9b6717.png","alt":"image.png","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"pSudsWPz0s","urlSource":"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_targets_003.png","urlOptimized":"/build/ff8c410b251590af858bbaf80f9b6717.webp"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Mathematically, specifying a ‘nugget’ value (or ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"XaWLn2RHKJ"},{"type":"inlineCode","value":"alpha","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Vyqvwm7oQK"},{"type":"text","value":" in GP terminology) is effectively adding a constant to the diagonal of the covariance matrix such that each observation has some level of variance with itself. Doing this allows flexibility for the mean of the predictions to ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"mxdIvy2w0o"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"not","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"SaFrThPQer"}],"key":"lswLgVgsQZ"},{"type":"text","value":" intersect all of the observation points-- if covariance is set to zero along the diagonal, then any estimation will be a ‘rubber sheeting’ that ensures the output surface prediction passes through the original data points. Of course, we can also specify covariance ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"nZqItcj8j6"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"per observation","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"eBgGBODHWw"}],"key":"pchONxwN2D"},{"type":"text","value":" rather than as a constant along the diagonal, and selectively ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"XeJ3VxLU1R"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"down weight","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"cFehXSGu5c"}],"key":"ta1LUoTuBC"},{"type":"text","value":" low confidence observations.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"kQLLpjSC1Z"}],"key":"OztKpdazrF"}],"key":"WBn9jCR2hq"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What I use Gaussian Processes for","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r4VvcheBt7"}],"identifier":"what-i-use-gaussian-processes-for","label":"What I use Gaussian Processes for","html_id":"what-i-use-gaussian-processes-for","implicit":true,"key":"mSCqk3qQLC"}],"key":"ihU90aclTc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%matplotlib inline","key":"yzbHNI5gXY"},{"type":"output","id":"ICRFtme3r_yimzT5-UzZF","data":[],"key":"cY4Fpx61sc"}],"key":"BQEiBJZXnE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom osgeo import osr\nimport pyproj\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel, RBF","key":"rI0STCd1RL"},{"type":"output","id":"SHU3pfnBQ0m1kzhMaHJMU","data":[],"key":"pFhMgC8mOl"}],"key":"akpDx99NAa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"rng = np.random.default_rng() # numpy sampling API\n#np.random.seed(1234)\n#np.random.seed(55)\nnp.random.seed(123)","key":"zAFP9hF3xX"},{"type":"output","id":"TPzZZ_3mcMCEOo7O8Ts9X","data":[],"key":"rsKIH0Ckzo"}],"key":"ntFT3sem7p"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"data = pd.HDFStore(\"./Z62SU06\" + \".h5\", 'r') # ICESat data\ncoordlist = data['coords'] # Needed for distance calculations / plotting\ndf_indices = data['indices'] # UUID to link to observations to slope retrievals\ndf_corr = data['corr'] # Filter and weight data\n\ngslope = np.load('./General_slopes.npy') # Sub ICESat footprint slope retrievals\nmean_slope = np.zeros(len(coordlist))\nstd_slope = np.zeros(len(coordlist))\n\n# Merge and aggregate slope\nfor i in tqdm(range(len(coordlist))):\n    sweights = np.array(df_corr.iloc[i].values)\n    weightidx = np.isfinite(sweights)\n    sweights = np.abs(sweights[weightidx])\n    sidx = df_indices.iloc[i].values[weightidx]\n    sidx = np.array(sidx, dtype=int)\n    mean_slope[i] = np.average(gslope[sidx], weights=sweights)\n    std_slope[i] = np.std(gslope[sidx])","key":"MQof38maBE"},{"type":"output","id":"Pk0Ci0_hsE4hZga7Fdwsq","data":[{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ae33b6b604ec4c5d830f236df557e5e1\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"  0%|          | 0/23359 [00:00<?, ?it/s]","content_type":"text/plain"}}}],"key":"EzSM5gL8la"}],"key":"ozm1Uc6m60"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This example is meant to be larger than a ‘toy’ dataset, but still small enough to interact with and demonstrate. Our input data consist of 23,359 observations (from the summer of 2006), which are within footprint slope retrievals from the first ICESat mission (which flew from 2003 to 2009, although laser power was strongest in the first three years).","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EI3YygZZ5I"}],"key":"zZkFtye84o"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Here’s what the data looks like:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Mj6mWWi9KP"}],"key":"iyBzmgYiEy"}],"key":"GF3D6gNZYx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fig, ax = plt.subplots()\nim = ax.scatter(coordlist.lon, coordlist.lat, c=mean_slope, s=std_slope**2)\nim.set_clim(min(mean_slope), max(mean_slope))\nfig.colorbar(im, ax=ax, label=\"Degrees of slope \\n (points scaled by retrieval variance)\")\nplt.show()","key":"ZeZwygLokk"},{"type":"output","id":"vBPltrFfKrZarFt28AnXM","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"7809e52609e3ddbec2e8e4baee69beb1","path":"/build/7809e52609e3ddbec2e8e4baee69beb1.png"},"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"}}}],"key":"OTEnZzIaP7"}],"key":"eKtuoou0FG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This is an interesting case of geostatistics because the sampling is quite odd-- the along track resolution is dense, but the across track resolution is sparse. Most of the code is actually setting up the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h4AypWeTnU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"source","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T4bi6YqMAk"}],"key":"KNI2UHigOh"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YQwIFmL7bZ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"target","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dZovGcuYEZ"}],"key":"o8nJXmzt9J"},{"type":"text","value":" coordinate systems for the model. The input observational data is irregularly spaced and in lat/lon coordinates, but we want our distance measurements to be in euclidean space and output to a grid.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VgRfSQ88e6"}],"key":"qGofVwE1NU"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We also want to ‘clip’ the results to a drainage basin. This is for two reasons-- first, the drainage basin boundries are of zero surface slope and provide a physically meaningful way to bound, divide, and tile the analysis. And second, covariance functions are expensive, so clipping and masking the edges of our area of interest helps things run faster... it also makes the output look good!","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"B9lFUvOq3z"}],"key":"ikO72DK0dK"}],"key":"AyxRXfSb7d"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Projection object\nds = osr.SpatialReference()\nds.ImportFromEPSG(3411) # polar projection\nds.ExportToProj4()\np2 = pyproj.Proj(ds.ExportToProj4())\n\n# Observations to meters from lat / lon\nX, Y = p2(np.array(coordlist.lon),\n          np.array(coordlist.lat))\n# To vector form sklearn\ncoords_m = np.ones((len(coordlist),2))\ncoords_m[:,0] = X\ncoords_m[:,1] = Y\n\n# Drainage basin boundries\nzwally = pd.read_csv('./GrnDrainageSystems_Ekholm.txt',\n                     sep=r\"\\s+\", names=['basin', 'lat','long'])\nbasin = '6.2'\nLL = zip(zwally[zwally.basin == float(basin)].long,\n         zwally[zwally.basin == float(basin)].lat)\nLL = list(LL)\n# Convert basin boundries to meters from lat / lon\npX, pY = p2(np.array(LL)[:,0],\n            np.array(LL)[:,1])\n# Polygon object for masking\nZ = mpl.path.Path(list(zip(pX, pY)))\n\n# Setting up kriging grid\nx1, y1 = np.meshgrid(np.r_[round(min(pX), -2) - 2500:round(max(pX), -2) + 2500:5000],\n                     np.r_[round(min(pY), -2) - 2500:round(max(pY), -2) + 2500:5000])\nkcoords = np.vstack([x1.ravel(),y1.ravel()]).T\n\n# Masking kriging grid\nZidx = Z.contains_points(kcoords[:])\ntarget_c = kcoords[Zidx] # Target coordinates\nx1.shape, len(target_c)","key":"HAVjgYTyCN"},{"type":"output","id":"N8MjkmYRO8ZjlwnplQN7N","data":[{"output_type":"execute_result","execution_count":6,"metadata":{},"data":{"text/plain":{"content":"((108, 110), 5600)","content_type":"text/plain"}}}],"key":"yF8Fazg7Qg"}],"key":"rjGV22FnLQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"At this point, we’ve setup a 2-km grid to predict on that’s 267 by 273 (a little over 500km per side), which after masking has 34,984 target coordinate locations to predict at. The actual Gaussian Process model specification is fairly short:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vvif39iBEg"}],"key":"AK3CYxOKNe"}],"key":"rXQUZcVHE5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ind = np.array(range(0,len(coordlist)), dtype=int)\nidxs = np.random.choice(ind, size=4000)","key":"CFwTcqCfQg"},{"type":"output","id":"74PMpFVsOKrXjKewT-p9x","data":[],"key":"ltlIe2DFBE"}],"key":"RVLVmJFMLQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\nnoise = WhiteKernel(noise_level=.5)\nrbf = 3 * RBF(length_scale=[80000,80000])\nkern = 2.0 * Matern(length_scale=[80000,80000], length_scale_bounds=(1e3, 1e6),\n                        nu=0.5)\ngp1 = GaussianProcessRegressor(kernel=rbf+kern+noise, alpha=std_slope[idxs]**2, optimizer=None).fit(coords_m[idxs], mean_slope[idxs])","key":"ZkJuInpYm2"},{"type":"output","id":"yPbObijNp_rhKwJBzItjC","data":[{"name":"stdout","output_type":"stream","text":"CPU times: user 7.48 s, sys: 984 ms, total: 8.46 s\nWall time: 1.29 s\n"}],"key":"GkKK1WXYq9"}],"key":"TDffK0upLo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We are specifying a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pMfjb50JuB"},{"type":"inlineCode","value":"Matern","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qNMRDdCyTj"},{"type":"text","value":" kernel to calculate and estimate our covarience, which is defined by the single parameter ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xFT9IQ7mTC"},{"type":"inlineCode","value":"nu","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mBYJNiFbAm"},{"type":"text","value":"; this parameter controls how smooth the fuctions are within our prior distribution of possible surface functions. It is set at 0.5 (i.e., ‘1 / 2’, equivelent the absolute exponential kernel) to allow rough, non-differentiable candidate functions; a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qyZCESxmYy"},{"type":"inlineCode","value":"nu=1.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b3pAQGTMg7"},{"type":"text","value":" (3 / 2) will select from functions that are once differentiable, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C7c5seSpmL"},{"type":"inlineCode","value":"nu=2.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kK326bQ6DT"},{"type":"text","value":" (5 / 2) will select from functions that are twice differentiable, and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hk2IZlKxkt"},{"type":"inlineCode","value":"nu=inf","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t3bpVH6dk7"},{"type":"text","value":" will converge to the infinately differentiable RBF kernel.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZMtfEcF4Qs"}],"key":"hYhWymzxpZ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uvuvjN1ygd"},{"type":"inlineCode","value":"length_scale","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"mP8Wbsyxft"},{"type":"text","value":" parameter would typically be optimized during model fit based on the spatial structure of the data... however, because our input data is heavily unbalanced (i.e., dense in the along track direction, sparse in the across track), hyper-parameter optimization sets a length scale that is too short, so it is fixed at 80km, with similar settings applied for the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oFcF4y2zv8"},{"type":"inlineCode","value":"length_scale_bounds","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"w4qUWNP1wc"},{"type":"text","value":".","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oaHhLQaD3I"}],"key":"BtQh63SzTl"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"After model specification, at ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"LGPqCphJzy"},{"type":"inlineCode","value":"fit","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"LZG2mz9S5p"},{"type":"text","value":" time we provide the coordinates, and slope values, along with an ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZpZy6Qjd0d"},{"type":"inlineCode","value":"alpha","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"m4rL83vAZI"},{"type":"text","value":" that is set to varience of slope retrieval (i.e., estimates of varience per observation).","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kSlizAbGQ1"}],"key":"B0S3UNxULf"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Doubling the number of observations (by adding a second summer of orbits) pushes the model fit time from under two minutes to about a half hour (although some of this is due to the higher memory footprint and pressure causing the OS to begin swapping to disk).","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"q8BOvLMBsN"}],"key":"zusXY3fgr4"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"We can get the mean surface estimate and target-to-target covariances with the following:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"gokjwAHWSa"}],"key":"ZP0DPhPSpJ"}],"key":"aExPEF75JT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\ny_mean, y_cov = gp1.predict(target_c, return_cov=True)","key":"OiqKy1JqT4"},{"type":"output","id":"BY6R9rBIRK9fzO42ibqbM","data":[{"name":"stdout","output_type":"stream","text":"CPU times: user 12.5 s, sys: 1.79 s, total: 14.3 s\nWall time: 2.09 s\n"}],"key":"QyQIDtIFcm"}],"key":"oNw6UDC98g"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The mean predictions need to be reshaped before plotting:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ts4sCMhaT0"}],"key":"Boo1ipoSBs"}],"key":"J14v7pOdED"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# slope\nresults1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  y_mean\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=0, vmax=10)\nplt.colorbar()\nplt.show()","key":"AcVDPWTUMy"},{"type":"output","id":"2hLPICBkJY4pQ_tJgrvbs","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"e4355c71dd40f2cba42c72ffa8a053c5","path":"/build/e4355c71dd40f2cba42c72ffa8a053c5.png"},"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"}}}],"key":"drWy46Nooh"}],"key":"BJdlDUYnSv"},{"type":"block","kind":"notebook-content","data":{"jp-MarkdownHeadingCollapsed":true},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What is kriging (formally)?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PJyz1FE0Y4"}],"identifier":"what-is-kriging-formally","label":"What is kriging (formally)?","html_id":"what-is-kriging-formally","implicit":true,"key":"X9hldBgtAI"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Mathematically, kriging is the Best Unbiased Linear ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lTwDgOPrjK"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Predictor","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aIBYv2b3LO"}],"key":"adMtOIr7Wb"},{"type":"text","value":" (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ZLnvNCjdCY"},{"type":"cite","identifier":"10.1007/bf02067214","label":"Journel_1977","kind":"narrative","position":{"start":{"line":3,"column":68},"end":{"line":3,"column":87}},"children":[{"type":"text","value":"Journel (1977)","key":"g3MzyIx4Lr"}],"enumerator":"4","key":"AiKRTEfLSe"},{"type":"text","value":" ; ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QbXOALaBRc"},{"type":"cite","identifier":"10.1007/bf00889887","label":"Cressie_1990","kind":"narrative","position":{"start":{"line":3,"column":90},"end":{"line":3,"column":109}},"children":[{"type":"text","value":"Cressie (1990)","key":"ZbDMLyDOaA"}],"enumerator":"10","key":"wP6aIhuJJe"},{"type":"text","value":" ; ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Cn4NvoW2sa"},{"type":"cite","identifier":"10.1080/00401706.1993.10485354","label":"Handcock_1993","kind":"narrative","position":{"start":{"line":3,"column":112},"end":{"line":3,"column":143}},"children":[{"type":"text","value":"Handcock & Stein (1993)","key":"Vyp6HvS2Jk"}],"enumerator":"11","key":"uTiuchrT71"},{"type":"text","value":"), and provides the Best Linear Unbiased Prediction, or BLUP; this means that kriging prediction is a linear combination of weighted observations at point p that minimizes prediction variance. Note that this is different (but related) to the Best Linear Unbiased ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lBNMeJ07Wk"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Estimation","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Hju35ofc5r"}],"key":"Bto2CP4oV1"},{"type":"text","value":", or BLUE... although when using kriging to ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ioZ1qU4dUS"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"estimate","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Wij8CnP5iJ"}],"key":"FSoGcg1WPJ"},{"type":"text","value":" a quantity, the results will be equivilent to the BLUE, which is why kriging is often conceptually understood as equivalent to spatially weighted generalized least squares regression. There are of course various flavors of kriging (simple, ordinary, universal, etc.) (see ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CvwA7QRRcz"},{"type":"cite","url":"10.1007/bf02067214","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Journel","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"U4LeWfpfCe"}],"kind":"narrative","label":"Journel_1977","identifier":"10.1007/bf02067214","enumerator":"4","key":"xJXPhUZQAA"},{"type":"text","value":"), but below is mathematical formulation for simple kriging (other flavors are similar):","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"y3ZSuLTT9A"}],"key":"cYHzcTvUtS"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Given N observations (s1…sn), and CovF(distance) covariance function, calculate D distances of N observations; note the convention of using ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"F40mY6reki"},{"type":"inlineMath","value":"t","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span>","key":"Ry4tdLYdBy"},{"type":"text","value":" for ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"FALQpGxuJF"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"target","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mWgbbuouNf"}],"key":"PoDrdzWx3i"},{"type":"text","value":" and ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"UiuoWMPhO3"},{"type":"inlineMath","value":"s","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span>","key":"uHGNZugDPe"},{"type":"text","value":" for ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"B76AqxQ7pU"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"source","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"yMiHL8U0X6"}],"key":"h5FEOwRdyt"},{"type":"text","value":":","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"O0eeVe9q7a"}],"key":"lRvjtO3hDE"},{"type":"math","value":"D(s_1 ... s_n, s_1 ... s_n) = D_{obs}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>D</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><msub><mi>s</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><msub><mi>s</mi><mn>1</mn></msub><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D(s_1 ... s_n, s_1 ... s_n) = D_{obs}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">...</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">...</span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">b</span><span class=\"mord mathnormal mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>","enumerator":"1","key":"G4vKIYjzza"},{"type":"math","value":"C_{obs} = f_{cov}(D_{obs}) \\quad \\textrm{or} \\quad C(\\mathbf{x}_n, \\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m)","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>C</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>c</mi><mi>o</mi><mi>v</mi></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow></msub><mo stretchy=\"false\">)</mo><mspace width=\"1em\"/><mtext>or</mtext><mspace width=\"1em\"/><mi>C</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold\">x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>k</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">x</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold\">x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">C_{obs} = f_{cov}(D_{obs}) \\quad \\textrm{or} \\quad C(\\mathbf{x}_n, \\mathbf{x}_m) = k(\\mathbf{x}_n, \\mathbf{x}_m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">b</span><span class=\"mord mathnormal mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">co</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">b</span><span class=\"mord mathnormal mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:1em;\"></span><span class=\"mord text\"><span class=\"mord textrm\">or</span></span><span class=\"mspace\" style=\"margin-right:1em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">m</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">m</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>","enumerator":"2","key":"YGVK1dxu8i"},{"type":"code","lang":"","value":"Dist(s1…sn, s1…sn) = Dobs  \t\t# square distance matrix\nCobs = CovF(Dobs)                       # observation covarience, i.e., source-to-source cov","position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"key":"j9evQoL7sN"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"This is what is happening here:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"CB0c1m1svz"}],"key":"OfxaDVKfuE"}],"key":"gU8rM70Jvc"},{"type":"code","lang":"python","value":"kern = 1.0 * Matern(length_scale=[80000,80000], length_scale_bounds=(1e3, 1e6),\n                        nu=0.5)\ngp1 = GaussianProcessRegressor(kernel=kern, alpha=std_slope**2, optimizer=None).fit(coords_m, mean_slope)","position":{"start":{"line":15,"column":1},"end":{"line":19,"column":1}},"key":"birDHtZV4X"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"We are selecting a covariance function (the Matern kernel), and then fitting that fuction to our observations on the basis of distance in order to estimate source-to-source cov","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"MwykIO3ep5"}],"key":"aThikf4CRi"}],"key":"fbdoms7WZg"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"Define p1…pn prediction nodes; calculate distances and Cpred (covariance of predictions):","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"Hd6AQIwTqu"}],"key":"bFyCiVfxRN"},{"type":"math","value":"D(t_1 ... t_n, t_1 ... t_n) = D_{pred}","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>D</mi><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><msub><mi>t</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><msub><mi>t</mi><mn>1</mn></msub><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi>D</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">D(t_1 ... t_n, t_1 ... t_n) = D_{pred}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">...</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">...</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">re</span><span class=\"mord mathnormal mtight\">d</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span></span>","enumerator":"3","key":"ExtWbIJ2Yf"},{"type":"math","value":"C_{pred} = f_{cov}(D_{pred})","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>C</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>c</mi><mi>o</mi><mi>v</mi></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">C_{pred} = f_{cov}(D_{pred})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">re</span><span class=\"mord mathnormal mtight\">d</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">co</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">re</span><span class=\"mord mathnormal mtight\">d</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>","enumerator":"4","key":"B9t2dSfEPL"},{"type":"code","lang":"","value":"Dist(t1…tn, t1…tn) = Dpred  \t\t# square distance matrix\nCpred = CovF(Dpred)                     # target covarience, i.e., target-to-target cov","position":{"start":{"line":28,"column":1},"end":{"line":29,"column":1}},"key":"QRrgV5DsbS"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Note that for above, we don’t actually know p1…pn ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"mmt2MP8l9h"},{"type":"emphasis","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"but we do know the locations","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"vzf0uOq9uw"}],"key":"HmpfWUpO9l"},{"type":"text","value":" of p1…pn, and hence we can calculate their distances, and given that our covariance function is a function of distances, we can ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"AoherKjsQN"},{"type":"strong","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"model","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"t5K8ytFiTa"}],"key":"E1e3wNmpnh"},{"type":"text","value":" the covariance of the predictions p1…pn as a function of distance.","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"vL6qm12z1z"}],"key":"jsFjFmkAmz"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"As mentioned above, kriging is equivalent to spatial least squares regression. Hence, we can retrieve our best estimate at locations p1…pn though least square regression—see below for useful equivalencies:","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"Hb6dEqM2X5"}],"key":"Gnb59W7OdL"},{"type":"code","lang":"","value":"Lstsq( s1…sn, t1…tn) == Lstsq( Cobs, Cobs_pred) == kriging_wieghts","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"qcCWLEC7DY"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"…where ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"NhNvwD2klA"},{"type":"inlineCode","value":"Cobs_pred","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ith9WM3uM0"},{"type":"text","value":" is the covariance between our observations and predictions, i.e., our target-to-source ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"h3AvtaGaAc"},{"type":"inlineCode","value":"cov","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"yXiR4Lo1hO"},{"type":"text","value":". Of course, we don’t know our predictions yet; all we have are our observations, and our observation covariances. However, we have defined a covariance function, ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"HrKXthrXNT"},{"type":"emphasis","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"based","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"a3VBKYvbAS"}],"key":"yBhxAKhMaz"}],"key":"oY3eKEhaDH"},{"type":"text","value":" on our observation covariances, that provides covariance as a function of distance; hence above we defined our prediction to prediction covariance.","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"yKrcODFHxH"}],"key":"xe4ArQFjZU"},{"type":"paragraph","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"strong","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"The kriging weights are used to weight a combination of our observations at each prediction location-- with the weights being determined by distance and the covariance structure. Our ‘mean’ prediction and target-to-target cov (and the steps above and below to retrieve them), happen at this call:","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"nXOFPaJQrV"}],"key":"e870zEI0jS"}],"key":"HJ1v9Y8pzk"},{"type":"code","lang":"python","value":"y_mean, y_cov = gp1.predict(target_c, return_cov=True)","position":{"start":{"line":41,"column":1},"end":{"line":43,"column":1}},"key":"kGJLdv8mrg"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"strong","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"(Note, this call is also implicitly calculating the source to target covariance!)","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"y4y2ub4q2E"}],"key":"uzAVnIrPJq"}],"key":"sSpxCaP8kw"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"Using the same assumption that covariance can be specified as a function of distance, we can get an estimate of covariance between our observations and prediction locations:","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"MCdvHGsx7E"}],"key":"u2k6n88viu"},{"type":"code","lang":"","value":"Cobs_pred = CovF(Dist(s1…sn, t1…tn)","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"CIF3TTJ63w"},{"type":"paragraph","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"…now we obtain the kriging weights by:","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"eGRk7tIIbl"}],"key":"aqWxAIVQZw"},{"type":"code","lang":"","value":"kriging_wieghts = dot(inv(Cobs), Cobs_pred)","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"iWcmPSc0xL"},{"type":"paragraph","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"Above we see the key computational constraint for kriging; we need to be able to invert an N by N observation covariance matrix.","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"jNO7zUmAVM"}],"key":"q7NtBeHosV"},{"type":"paragraph","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Given that the regression/kriging weights are identical for both the observations and their covariances, we now have enough information to make a kriging/Lstsq’s prediction of t1…tn:","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"jLCkGeTVBm"}],"key":"TzgM1woWtm"},{"type":"code","lang":"","value":"t1…tn =  dot(kriging_wieghts.T, s1…sn)","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"WYXbutex7O"},{"type":"paragraph","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"We can calculate partial correlation/covariance, which is equivalent to the covariance of the residual error from kriging/regression. We get conditional covariance with:","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"g5DPu3GSmy"}],"key":"wdqtZGWzhm"},{"type":"code","lang":"","value":"Cpred – dot(dot(Cobs_pred.T, inv(Cobs)), Cobs_pred)","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"M90ig2E0ED"},{"type":"paragraph","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"children":[{"type":"text","value":"The above gives the estimation error covariance, and allows for simulation, i.e., ","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"aNOhhzDACW"},{"type":"emphasis","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"children":[{"type":"text","value":"prediction","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"AYXVhO9Ijb"}],"key":"GIRrkZ12mI"},{"type":"text","value":". Specifically, we can preform simulation via Cholesky decomposition* using the derived mean target predictions, and the target-to-target covariance:","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"XXK5ZVh0OY"}],"key":"iVaFHL38GE"},{"type":"paragraph","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"(from ","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"WUafe1iRfj"},{"type":"link","url":"https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/_gpr.py","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"sklearn gaussian process module","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"EOzrwAQ9dL"}],"urlSource":"https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/_gpr.py","data":{"kind":"file","org":"scikit-learn","repo":"scikit-learn","reference":"main","file":"sklearn/gaussian_process/_gpr.py","raw":"https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/sklearn/gaussian_process/_gpr.py"},"internal":false,"protocol":"github","key":"dWyowaXIwx"},{"type":"text","value":"):","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"fT6c1gd1eT"}],"key":"GqxQTJSfSe"},{"type":"code","lang":"python","value":"    def sample_y(self, X, n_samples=1, random_state=0):\n        \"\"\"Draw samples from Gaussian process and evaluate at X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples_X, n_features) or list of object\n            Query points where the GP is evaluated.\n\n        n_samples : int, default=1\n            Number of samples drawn from the Gaussian process per query point.\n\n        random_state : int, RandomState instance or None, default=0\n            Determines random number generation to randomly draw samples.\n            Pass an int for reproducible results across multiple function\n            calls.\n            See :term:`Glossary <random_state>`.\n\n        Returns\n        -------\n        y_samples : ndarray of shape (n_samples_X, n_samples), or \\\n            (n_samples_X, n_targets, n_samples)\n            Values of n_samples samples drawn from Gaussian process and\n            evaluated at query points.\n        \"\"\"\n        rng = check_random_state(random_state)\n\n        y_mean, y_cov = self.predict(X, return_cov=True)\n        if y_mean.ndim == 1:\n            y_samples = rng.multivariate_normal(y_mean, y_cov, n_samples).T\n        else:\n            y_samples = [\n                rng.multivariate_normal(\n                    y_mean[:, target], y_cov[..., target], n_samples\n                ).T[:, np.newaxis]\n                for target in range(y_mean.shape[1])\n            ]\n            y_samples = np.hstack(y_samples)\n        return y_samples","position":{"start":{"line":68,"column":1},"end":{"line":107,"column":1}},"key":"AToMjy1mga"},{"type":"paragraph","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"Several things are apparent for the kriging description above, that highlight both the strengths and weakness of the approach. First and foremost, ","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"ypYYlQtBWr"},{"type":"emphasis","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"the approach is completely dependent on specification of a proper covariance function","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"RSWGmAfH22"}],"key":"VrWi9prbcq"},{"type":"text","value":". ","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"RrQiidCbBu"},{"type":"strong","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"The covariance function must be invertible; that is it must be positive definite (Genton, 2002)","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"Nzv6CHXehL"}],"key":"ckNJY4alnw"},{"type":"text","value":", and the machine that runs the computation must be capable of inverting a N by N matrix. In practice, the N by N inversion can be relaxed if only the ‘best’ prediction is desired (since it can be derived via lstsq calculation); ","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"H5n9bMiaPv"},{"type":"emphasis","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"however, simulation and error estimation requires inversion of an N by N matrix.","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"mWvZNyK0OR"}],"key":"ax29jWyc6H"},{"type":"text","value":" ","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"g7vg54nPn3"},{"type":"strong","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"In practice, empirical covariance almost always provides a non-invertible (not positive definite) gram matrix (","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"kwSvUNk2Eh"},{"type":"cite","identifier":"10.1080/00401706.1993.10485354","label":"Handcock_1993","kind":"narrative","position":{"start":{"line":109,"column":761},"end":{"line":109,"column":792}},"children":[{"type":"text","value":"Handcock & Stein (1993)","key":"eHAFkSnA3o"}],"enumerator":"11","key":"fhU8GRPb2K"},{"type":"text","value":"); further, a model of covariance is needed to estimate source to target and target to target covariance—so estimation of a covariance function, either via a variogram or by other methods, is always required for geostatistical interpolation and simulation.","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"bZk2ZDb92D"}],"key":"sZcXe7uUg7"},{"type":"text","value":" The error estimation that kriging gives per prediction is error estimation on the assumption that modeled covariance structure is true! Kriging has been characterized as the Best Unbiased Linear Predictor (BULP)… which it is, for a given covariance function. Swapping covariance functions gives different and competing BLUPs, which then need to be evaluated via inter-model comparison (","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"l0k3zh7dgh"},{"type":"cite","url":"https://doi.org/10.1162/neco.1992.4.3.415","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"MacKay (1992)","key":"KvOqFPv1oU"}],"kind":"narrative","label":"MacKay_1992","identifier":"https://doi.org/10.1162/neco.1992.4.3.415","enumerator":"12","key":"JPNuQ9GJXg"},{"type":"text","value":"). In short, the kriging error and uncertainty estimation is not the absolute error estimate for the surface, and probability estimation of a prediction must be obtained with other models to give realistic confidence bounds.","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"EoIdlJSphn"}],"key":"XPCtrzCr65"},{"type":"paragraph","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"children":[{"type":"text","value":"*Traditional kriging involves estimation of a variogram, however, the variogram is simply a way of producing a covariance function that produces a positive definite gram matrix of covariances (Genton, 2002). Since we know that our covariance function produces a positive definite gram matrix of covariances, we use cholesky because ","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"key":"wXxJ5kWrjU"},{"type":"link","url":"https://numpy.org/doc/2.2/reference/random/generated/numpy.random.Generator.multivariate_normal.html#numpy.random.Generator.multivariate_normal","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"children":[{"type":"text","value":"it is faster than SVD or eigen decomposition","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"key":"H7wNPdfVI8"}],"urlSource":"https://numpy.org/doc/2.2/reference/random/generated/numpy.random.Generator.multivariate_normal.html#numpy.random.Generator.multivariate_normal","key":"hth1KOWzMt"},{"type":"text","value":".","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"key":"nScvx8Zw1v"}],"key":"mooOsd5Hvx"}],"key":"rjctTY6jpy"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Why Prediction?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HGPCfMxuo2"}],"identifier":"why-prediction","label":"Why Prediction?","html_id":"why-prediction","implicit":true,"key":"SkMDTbbrAI"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Slope is an important variable for glaciologists, but it also doesn’t vary much on the icesheet-- it goes from about 2 degrees at the edge where it’s ‘steep’, before gradually dropping down to 1 in the interior and eventually 0 at the drainage boundary. However, things change when the surface is crevassed; cracks present a range of surface heights and angles, and generally those retrievals are ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hBFFMWkUNQ"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"not","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Rp0643HWkE"}],"key":"oRGKBtW6Hv"},{"type":"text","value":" flat.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Lf9kgp4JiH"}],"key":"vEX0Hsvc3v"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"So if we are looking for crevasses, and we know that the ‘background’ slope of the ice sheet is 2 degrees of slope or lower, it would be nice to know when that slope is exceeded. Unfortunately, our mean prediction of surface slope is smoothing the variation in slope retrievals-- if we have a distribution of slope values, the best prediction will trend towards the mean, resulting in surface that suppresses variation we know is present.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kd3UcGRxnd"}],"key":"LIUWKctxuD"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"What we can do though, is simulate possible surfaces, and count how often we exceed 2 degrees of slope for a given location:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"wT2xr1fGeW"}],"key":"WivXOJFuPN"}],"key":"g9iqpd3OjM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\nrealizations = rng.multivariate_normal(y_mean, y_cov, size=100, method='cholesky')","key":"mFFUTugzjF"},{"type":"output","id":"Q0dE4tCUj1r-3NSfi-eXa","data":[{"name":"stdout","output_type":"stream","text":"CPU times: user 8.21 s, sys: 313 ms, total: 8.52 s\nWall time: 1.12 s\n"}],"key":"jP6V7VhbUm"}],"key":"yHtB9fXLCi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"results1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  realizations[5]\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=2)\nplt.colorbar()\nplt.show()","key":"VwU5GtJMRn"},{"type":"output","id":"0a5qZ8XNe_dajSuq9_Oi1","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"16054cd9d3452404ea4c0d28a5ebc991","path":"/build/16054cd9d3452404ea4c0d28a5ebc991.png"},"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"}}}],"key":"kolWbaaoAc"}],"key":"wgD4MUWeUf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"results1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  realizations[9]\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1), vmin=2)\nplt.colorbar()\nplt.show()","key":"Hnk5r4jl9d"},{"type":"output","id":"6K8j64ofh3Rhr6mSsZF61","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"5613a94aefe71a55d1b42b8e85bc75d9","path":"/build/5613a94aefe71a55d1b42b8e85bc75d9.png"},"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"}}}],"key":"fAwnhNYJCE"}],"key":"oav8SejSai"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"occurance = np.zeros(5600, dtype=np.int64)\nfor i in realizations:\n    occurance += i > 3","key":"cWVwRZj3Gu"},{"type":"output","id":"NT3O1UCicG9ovN8s0hMO3","data":[],"key":"JOk7RVn2C3"}],"key":"iPkwY8kV6S"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"","key":"ouqFNsnWKZ"},{"type":"output","id":"UL87g3u8NYv9yNKai7Ywk","data":[],"key":"gLz2V7fym9"}],"key":"fVulMGzWbH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"results1 = np.ones(len(kcoords)) *np.nan\nresults1[Zidx] =  occurance\nshow_res1 = results1.reshape((x1.shape))\nplt.imshow(np.flipud(show_res1)) #, norm=mpl.colors.LogNorm())\nticks = np.array([0,20,40,60,80])\nyticks = np.array([100,80,60,40,20])\nplt.xticks(ticks[1:],(ticks[0:-1]*5))\nplt.yticks(yticks,(ticks*5))\nplt.colorbar()\nplt.show()","key":"liNZKDeV92"},{"type":"output","id":"TwfosROyLIwX0XEtNG6p6","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"bd2f9c7b57e429a89e58d05f8c66d1c8","path":"/build/bd2f9c7b57e429a89e58d05f8c66d1c8.png"},"text/plain":{"content":"<Figure size 640x480 with 2 Axes>","content_type":"text/plain"}}}],"key":"qiOcEIbWmN"}],"key":"XcIDQWOgTa"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Covarience Kernels","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"frPo6RZLk0"}],"identifier":"covarience-kernels","label":"Covarience Kernels","html_id":"covarience-kernels","implicit":true,"key":"xkBP0nDI9V"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Observe the following GP kernels (i.e., covarience models), fitted to the same data; note that every model consistently ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"BNJbD0gaK1"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"predicts","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Hu7N20jHHN"}],"key":"Usm3gvgM6T"},{"type":"text","value":" an additional drop ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Av6T7kDvQZ"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"beyond the observations","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"IhIixjJIpj"}],"key":"CGhI6kyHum"},{"type":"text","value":" between x=4 and x=5; interpolation using most estimators cannot exceed the min/max of the observation space!","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"j3JatFHgn8"}],"key":"dDhYCAEBWq"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"image","url":"/build/640cbbfcbbe6456d9de17be99a0b8710.png","alt":"gp1.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"LGBEUiKb04","urlSource":"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_001.png","urlOptimized":"/build/640cbbfcbbe6456d9de17be99a0b8710.webp"},{"type":"text","value":"\n","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"RQcAPWDiFg"},{"type":"image","url":"/build/02c12096c474beb07b01aa952e6d7321.png","alt":"gp1.png","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mmD3egw8lA","urlSource":"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_002.png","urlOptimized":"/build/02c12096c474beb07b01aa952e6d7321.webp"}],"key":"eU5cOIwW2u"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Note that the next kernel is modeling period functions (applicable to basin and range features)","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"xpw6xY4FcR"}],"key":"s5mwFu3GTd"}],"key":"pW1pGRT7Zh"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"image","url":"/build/d5647b7711a1f652476f0e28745cc0ab.png","alt":"gp1.png","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"bI5pScoror","urlSource":"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_003.png","urlOptimized":"/build/d5647b7711a1f652476f0e28745cc0ab.webp"},{"type":"text","value":"\n","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"PXrYA6QnCq"},{"type":"image","url":"/build/3a9cdfda5457eba4bcbf6bf31a4c9441.png","alt":"gp1.png","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"HEYuwWm2z8","urlSource":"https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_005.png","urlOptimized":"/build/3a9cdfda5457eba4bcbf6bf31a4c9441.webp"}],"key":"s90NHR3LPA"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Note that one major benefit to the GP kernel formulation is that ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"o9RZMJtdPy"},{"type":"link","url":"https://towardsdatascience.com/gaussian-process-kernels-96bafb4dd63e/","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"kernels can be ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"NkUBMwSlFz"},{"type":"emphasis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"combined","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Q8gbItGasS"}],"key":"W2CJr0BgHy"}],"urlSource":"https://towardsdatascience.com/gaussian-process-kernels-96bafb4dd63e/","key":"pt9aXhiFjd"},{"type":"text","value":". This approach models kernels to account for the non-stationarity of the processes within the kernel formulation, rather than seperating out a","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Yj82Ocdm9S"}],"key":"w8nSpIhw9O"}],"key":"Z2fnGFouBL"}],"key":"R23mRmoOtn"},"references":{"cite":{"order":["Williams_1998","Rasmussen_2005","Matheron_1973","Journel_1977","genton","https://doi.org/10.48550/arxiv.1302.4245","Cressie_1999","Mat_rn_1986","https://doi.org/10.48550/arxiv.1503.01057","Cressie_1990","Handcock_1993","MacKay_1992"],"data":{"Williams_1998":{"label":"Williams_1998","enumerator":"1","doi":"10.1007/978-94-011-5014-9_23","html":"Williams, C. K. I. (1998). Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond. In <i>Learning in Graphical Models</i> (pp. 599–621). Springer Netherlands. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1007/978-94-011-5014-9_23\">10.1007/978-94-011-5014-9_23</a>","url":"https://doi.org/10.1007/978-94-011-5014-9_23"},"Rasmussen_2005":{"label":"Rasmussen_2005","enumerator":"2","doi":"10.7551/mitpress/3206.001.0001","html":"Rasmussen, C. E., & Williams, C. K. I. (2005). <i>Gaussian Processes for Machine Learning</i>. The MIT Press. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.7551/mitpress/3206.001.0001\">10.7551/mitpress/3206.001.0001</a>","url":"https://doi.org/10.7551/mitpress/3206.001.0001"},"Matheron_1973":{"label":"Matheron_1973","enumerator":"3","doi":"10.2307/1425829","html":"Matheron, G. (1973). The intrinsic random functions and their applications. <i>Advances in Applied Probability</i>, <i>5</i>(3), 439–468. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.2307/1425829\">10.2307/1425829</a>","url":"https://doi.org/10.2307/1425829"},"Journel_1977":{"label":"Journel_1977","enumerator":"4","doi":"10.1007/bf02067214","html":"Journel, A. G. (1977). Kriging in terms of projections. <i>Mathematical Geology</i>, <i>9</i>(6), 563–586. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1007/bf02067214\">10.1007/bf02067214</a>","url":"https://doi.org/10.1007/bf02067214"},"genton":{"label":"genton","enumerator":"5","doi":"10.5555/944790.944815","html":"Genton, M. G. (2002). Classes of kernels for machine learning: a statistics perspective. <i>J. Mach. Learn. Res.</i>, <i>2</i>, 299–312. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.5555/944790.944815\">10.5555/944790.944815</a>","url":"https://doi.org/10.5555/944790.944815"},"https://doi.org/10.48550/arxiv.1302.4245":{"label":"https://doi.org/10.48550/arxiv.1302.4245","enumerator":"6","doi":"10.48550/ARXIV.1302.4245","html":"Wilson, A. G., & Adams, R. P. (2013). <i>Gaussian Process Kernels for Pattern Discovery and Extrapolation</i>. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1302.4245\">10.48550/ARXIV.1302.4245</a>","url":"https://doi.org/10.48550/ARXIV.1302.4245"},"Cressie_1999":{"label":"Cressie_1999","enumerator":"7","doi":"10.1080/01621459.1999.10473885","html":"Cressie, N., & Huang, H.-C. (1999). Classes of Nonseparable, Spatio-Temporal Stationary Covariance Functions. <i>Journal of the American Statistical Association</i>, <i>94</i>(448), 1330–1339. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1080/01621459.1999.10473885\">10.1080/01621459.1999.10473885</a>","url":"https://doi.org/10.1080/01621459.1999.10473885"},"Mat_rn_1986":{"label":"Mat_rn_1986","enumerator":"8","doi":"10.1007/978-1-4615-7892-5","html":"Matérn, B. (1986). Spatial Variation. In <i>Lecture Notes in Statistics</i>. Springer New York. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1007/978-1-4615-7892-5\">10.1007/978-1-4615-7892-5</a>","url":"https://doi.org/10.1007/978-1-4615-7892-5"},"https://doi.org/10.48550/arxiv.1503.01057":{"label":"https://doi.org/10.48550/arxiv.1503.01057","enumerator":"9","doi":"10.48550/ARXIV.1503.01057","html":"Wilson, A. G., & Nickisch, H. (2015). <i>Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1503.01057\">10.48550/ARXIV.1503.01057</a>","url":"https://doi.org/10.48550/ARXIV.1503.01057"},"Cressie_1990":{"label":"Cressie_1990","enumerator":"10","doi":"10.1007/bf00889887","html":"Cressie, N. (1990). The origins of kriging. <i>Mathematical Geology</i>, <i>22</i>(3), 239–252. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1007/bf00889887\">10.1007/bf00889887</a>","url":"https://doi.org/10.1007/bf00889887"},"Handcock_1993":{"label":"Handcock_1993","enumerator":"11","doi":"10.1080/00401706.1993.10485354","html":"Handcock, M. S., & Stein, M. L. (1993). A Bayesian Analysis of Kriging. <i>Technometrics</i>, <i>35</i>(4), 403–410. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1080/00401706.1993.10485354\">10.1080/00401706.1993.10485354</a>","url":"https://doi.org/10.1080/00401706.1993.10485354"},"MacKay_1992":{"label":"MacKay_1992","enumerator":"12","doi":"10.1162/neco.1992.4.3.415","html":"MacKay, D. J. C. (1992). Bayesian Interpolation. <i>Neural Computation</i>, <i>4</i>(3), 415–447. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1162/neco.1992.4.3.415\">10.1162/neco.1992.4.3.415</a>","url":"https://doi.org/10.1162/neco.1992.4.3.415"}}}},"footer":{"navigation":{"prev":{"title":"Coding & Writing Sample","url":"/gaussianprocesses","group":"Coding Sample"}}},"domain":"http://localhost:3001"}